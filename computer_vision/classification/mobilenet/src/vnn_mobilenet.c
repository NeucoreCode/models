/****************************************************************************
*   Generated by ACUITY 3.11.0
*   Match ovxlib 1.0.8
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_mobilenet.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        _node->uid = (uint32_t)_uid; \
        if( NULL == _node ) {\
            goto error;\
        }\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (30)
#define NET_NORM_TENSOR_NUM     (2)
#define NET_CONST_TENSOR_NUM    (56)
#define NET_VIRTUAL_TENSOR_NUM  (30)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM + 32)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static void load_hw_config
    (
    vsi_nn_context_t ctx
    )
{
    ctx->config.evis.ver = VSI_NN_HW_EVIS_2;
    strncpy(ctx->config.target_name, "VIP8000", VSI_NN_MAX_TARGET_NAME);
}

static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    VSILOGI("Read %d data.", ret);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateMobilenet
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx
    )
{
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;

    uint32_t   shape_1[] = { 1, 1, 1000, 1 };


    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
        load_hw_config(ctx);
    }
    else
    {
        ctx = in_ctx;
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 1 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - conv1_1_relu1_4
      var       - node[0]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[224, 224, 3, 1]]
      out_shape - [[112, 112, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_CONV_RELU, 3, 1, 4);
    node[0]->nn_param.conv2d.ksize[0] = 3;
    node[0]->nn_param.conv2d.ksize[1] = 3;
    node[0]->nn_param.conv2d.weights = 32;
    node[0]->nn_param.conv2d.stride[0] = 2;
    node[0]->nn_param.conv2d.stride[1] = 2;
    node[0]->nn_param.conv2d.pad[0] = 1;
    node[0]->nn_param.conv2d.pad[1] = 1;
    node[0]->nn_param.conv2d.pad[2] = 1;
    node[0]->nn_param.conv2d.pad[3] = 1;
    node[0]->nn_param.conv2d.group = 1;
    node[0]->nn_param.conv2d.dilation[0] = 1;
    node[0]->nn_param.conv2d.dilation[1] = 1;
    node[0]->vx_param.has_relu = TRUE;
    node[0]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[0]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[0]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2_1/dw_5_relu2_1/dw_8
      var       - node[1]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[112, 112, 32, 1]]
      out_shape - [[112, 112, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_CONV_RELU, 3, 1, 8);
    node[1]->nn_param.conv2d.ksize[0] = 3;
    node[1]->nn_param.conv2d.ksize[1] = 3;
    node[1]->nn_param.conv2d.weights = 32;
    node[1]->nn_param.conv2d.stride[0] = 1;
    node[1]->nn_param.conv2d.stride[1] = 1;
    node[1]->nn_param.conv2d.pad[0] = 1;
    node[1]->nn_param.conv2d.pad[1] = 1;
    node[1]->nn_param.conv2d.pad[2] = 1;
    node[1]->nn_param.conv2d.pad[3] = 1;
    node[1]->nn_param.conv2d.group = 1;
    node[1]->nn_param.conv2d.dilation[0] = 1;
    node[1]->nn_param.conv2d.dilation[1] = 1;
    node[1]->vx_param.has_relu = TRUE;
    node[1]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[1]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[1]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2_1/sep_9_relu2_1/sep_12
      var       - node[2]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[112, 112, 32, 1]]
      out_shape - [[112, 112, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_CONV_RELU, 3, 1, 12);
    node[2]->nn_param.conv2d.ksize[0] = 1;
    node[2]->nn_param.conv2d.ksize[1] = 1;
    node[2]->nn_param.conv2d.weights = 64;
    node[2]->nn_param.conv2d.stride[0] = 1;
    node[2]->nn_param.conv2d.stride[1] = 1;
    node[2]->nn_param.conv2d.pad[0] = 0;
    node[2]->nn_param.conv2d.pad[1] = 0;
    node[2]->nn_param.conv2d.pad[2] = 0;
    node[2]->nn_param.conv2d.pad[3] = 0;
    node[2]->nn_param.conv2d.group = 1;
    node[2]->nn_param.conv2d.dilation[0] = 1;
    node[2]->nn_param.conv2d.dilation[1] = 1;
    node[2]->vx_param.has_relu = TRUE;
    node[2]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[2]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[2]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2_2/dw_13_relu2_2/dw_16
      var       - node[3]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[112, 112, 64, 1]]
      out_shape - [[56, 56, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_CONV_RELU, 3, 1, 16);
    node[3]->nn_param.conv2d.ksize[0] = 3;
    node[3]->nn_param.conv2d.ksize[1] = 3;
    node[3]->nn_param.conv2d.weights = 64;
    node[3]->nn_param.conv2d.stride[0] = 2;
    node[3]->nn_param.conv2d.stride[1] = 2;
    node[3]->nn_param.conv2d.pad[0] = 1;
    node[3]->nn_param.conv2d.pad[1] = 1;
    node[3]->nn_param.conv2d.pad[2] = 1;
    node[3]->nn_param.conv2d.pad[3] = 1;
    node[3]->nn_param.conv2d.group = 1;
    node[3]->nn_param.conv2d.dilation[0] = 1;
    node[3]->nn_param.conv2d.dilation[1] = 1;
    node[3]->vx_param.has_relu = TRUE;
    node[3]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[3]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[3]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2_2/sep_17_relu2_2/sep_20
      var       - node[4]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 64, 1]]
      out_shape - [[56, 56, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV_RELU, 3, 1, 20);
    node[4]->nn_param.conv2d.ksize[0] = 1;
    node[4]->nn_param.conv2d.ksize[1] = 1;
    node[4]->nn_param.conv2d.weights = 128;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 0;
    node[4]->nn_param.conv2d.pad[1] = 0;
    node[4]->nn_param.conv2d.pad[2] = 0;
    node[4]->nn_param.conv2d.pad[3] = 0;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->vx_param.has_relu = TRUE;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv3_1/dw_21_relu3_1/dw_24
      var       - node[5]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 128, 1]]
      out_shape - [[56, 56, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_CONV_RELU, 3, 1, 24);
    node[5]->nn_param.conv2d.ksize[0] = 3;
    node[5]->nn_param.conv2d.ksize[1] = 3;
    node[5]->nn_param.conv2d.weights = 128;
    node[5]->nn_param.conv2d.stride[0] = 1;
    node[5]->nn_param.conv2d.stride[1] = 1;
    node[5]->nn_param.conv2d.pad[0] = 1;
    node[5]->nn_param.conv2d.pad[1] = 1;
    node[5]->nn_param.conv2d.pad[2] = 1;
    node[5]->nn_param.conv2d.pad[3] = 1;
    node[5]->nn_param.conv2d.group = 1;
    node[5]->nn_param.conv2d.dilation[0] = 1;
    node[5]->nn_param.conv2d.dilation[1] = 1;
    node[5]->vx_param.has_relu = TRUE;
    node[5]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[5]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[5]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv3_1/sep_25_relu3_1/sep_28
      var       - node[6]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 128, 1]]
      out_shape - [[56, 56, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_CONV_RELU, 3, 1, 28);
    node[6]->nn_param.conv2d.ksize[0] = 1;
    node[6]->nn_param.conv2d.ksize[1] = 1;
    node[6]->nn_param.conv2d.weights = 128;
    node[6]->nn_param.conv2d.stride[0] = 1;
    node[6]->nn_param.conv2d.stride[1] = 1;
    node[6]->nn_param.conv2d.pad[0] = 0;
    node[6]->nn_param.conv2d.pad[1] = 0;
    node[6]->nn_param.conv2d.pad[2] = 0;
    node[6]->nn_param.conv2d.pad[3] = 0;
    node[6]->nn_param.conv2d.group = 1;
    node[6]->nn_param.conv2d.dilation[0] = 1;
    node[6]->nn_param.conv2d.dilation[1] = 1;
    node[6]->vx_param.has_relu = TRUE;
    node[6]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[6]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv3_2/dw_29_relu3_2/dw_32
      var       - node[7]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 128, 1]]
      out_shape - [[28, 28, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_CONV_RELU, 3, 1, 32);
    node[7]->nn_param.conv2d.ksize[0] = 3;
    node[7]->nn_param.conv2d.ksize[1] = 3;
    node[7]->nn_param.conv2d.weights = 128;
    node[7]->nn_param.conv2d.stride[0] = 2;
    node[7]->nn_param.conv2d.stride[1] = 2;
    node[7]->nn_param.conv2d.pad[0] = 1;
    node[7]->nn_param.conv2d.pad[1] = 1;
    node[7]->nn_param.conv2d.pad[2] = 1;
    node[7]->nn_param.conv2d.pad[3] = 1;
    node[7]->nn_param.conv2d.group = 1;
    node[7]->nn_param.conv2d.dilation[0] = 1;
    node[7]->nn_param.conv2d.dilation[1] = 1;
    node[7]->vx_param.has_relu = TRUE;
    node[7]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[7]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[7]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv3_2/sep_33_relu3_2/sep_36
      var       - node[8]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 128, 1]]
      out_shape - [[28, 28, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_CONV_RELU, 3, 1, 36);
    node[8]->nn_param.conv2d.ksize[0] = 1;
    node[8]->nn_param.conv2d.ksize[1] = 1;
    node[8]->nn_param.conv2d.weights = 256;
    node[8]->nn_param.conv2d.stride[0] = 1;
    node[8]->nn_param.conv2d.stride[1] = 1;
    node[8]->nn_param.conv2d.pad[0] = 0;
    node[8]->nn_param.conv2d.pad[1] = 0;
    node[8]->nn_param.conv2d.pad[2] = 0;
    node[8]->nn_param.conv2d.pad[3] = 0;
    node[8]->nn_param.conv2d.group = 1;
    node[8]->nn_param.conv2d.dilation[0] = 1;
    node[8]->nn_param.conv2d.dilation[1] = 1;
    node[8]->vx_param.has_relu = TRUE;
    node[8]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[8]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[8]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv4_1/dw_37_relu4_1/dw_40
      var       - node[9]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_CONV_RELU, 3, 1, 40);
    node[9]->nn_param.conv2d.ksize[0] = 3;
    node[9]->nn_param.conv2d.ksize[1] = 3;
    node[9]->nn_param.conv2d.weights = 256;
    node[9]->nn_param.conv2d.stride[0] = 1;
    node[9]->nn_param.conv2d.stride[1] = 1;
    node[9]->nn_param.conv2d.pad[0] = 1;
    node[9]->nn_param.conv2d.pad[1] = 1;
    node[9]->nn_param.conv2d.pad[2] = 1;
    node[9]->nn_param.conv2d.pad[3] = 1;
    node[9]->nn_param.conv2d.group = 1;
    node[9]->nn_param.conv2d.dilation[0] = 1;
    node[9]->nn_param.conv2d.dilation[1] = 1;
    node[9]->vx_param.has_relu = TRUE;
    node[9]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[9]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[9]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv4_1/sep_41_relu4_1/sep_44
      var       - node[10]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_CONV_RELU, 3, 1, 44);
    node[10]->nn_param.conv2d.ksize[0] = 1;
    node[10]->nn_param.conv2d.ksize[1] = 1;
    node[10]->nn_param.conv2d.weights = 256;
    node[10]->nn_param.conv2d.stride[0] = 1;
    node[10]->nn_param.conv2d.stride[1] = 1;
    node[10]->nn_param.conv2d.pad[0] = 0;
    node[10]->nn_param.conv2d.pad[1] = 0;
    node[10]->nn_param.conv2d.pad[2] = 0;
    node[10]->nn_param.conv2d.pad[3] = 0;
    node[10]->nn_param.conv2d.group = 1;
    node[10]->nn_param.conv2d.dilation[0] = 1;
    node[10]->nn_param.conv2d.dilation[1] = 1;
    node[10]->vx_param.has_relu = TRUE;
    node[10]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[10]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[10]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv4_2/dw_45_relu4_2/dw_48
      var       - node[11]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[14, 14, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_CONV_RELU, 3, 1, 48);
    node[11]->nn_param.conv2d.ksize[0] = 3;
    node[11]->nn_param.conv2d.ksize[1] = 3;
    node[11]->nn_param.conv2d.weights = 256;
    node[11]->nn_param.conv2d.stride[0] = 2;
    node[11]->nn_param.conv2d.stride[1] = 2;
    node[11]->nn_param.conv2d.pad[0] = 1;
    node[11]->nn_param.conv2d.pad[1] = 1;
    node[11]->nn_param.conv2d.pad[2] = 1;
    node[11]->nn_param.conv2d.pad[3] = 1;
    node[11]->nn_param.conv2d.group = 1;
    node[11]->nn_param.conv2d.dilation[0] = 1;
    node[11]->nn_param.conv2d.dilation[1] = 1;
    node[11]->vx_param.has_relu = TRUE;
    node[11]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[11]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[11]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv4_2/sep_49_relu4_2/sep_52
      var       - node[12]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 256, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_CONV_RELU, 3, 1, 52);
    node[12]->nn_param.conv2d.ksize[0] = 1;
    node[12]->nn_param.conv2d.ksize[1] = 1;
    node[12]->nn_param.conv2d.weights = 512;
    node[12]->nn_param.conv2d.stride[0] = 1;
    node[12]->nn_param.conv2d.stride[1] = 1;
    node[12]->nn_param.conv2d.pad[0] = 0;
    node[12]->nn_param.conv2d.pad[1] = 0;
    node[12]->nn_param.conv2d.pad[2] = 0;
    node[12]->nn_param.conv2d.pad[3] = 0;
    node[12]->nn_param.conv2d.group = 1;
    node[12]->nn_param.conv2d.dilation[0] = 1;
    node[12]->nn_param.conv2d.dilation[1] = 1;
    node[12]->vx_param.has_relu = TRUE;
    node[12]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[12]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[12]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_1/dw_53_relu5_1/dw_56
      var       - node[13]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_CONV_RELU, 3, 1, 56);
    node[13]->nn_param.conv2d.ksize[0] = 3;
    node[13]->nn_param.conv2d.ksize[1] = 3;
    node[13]->nn_param.conv2d.weights = 512;
    node[13]->nn_param.conv2d.stride[0] = 1;
    node[13]->nn_param.conv2d.stride[1] = 1;
    node[13]->nn_param.conv2d.pad[0] = 1;
    node[13]->nn_param.conv2d.pad[1] = 1;
    node[13]->nn_param.conv2d.pad[2] = 1;
    node[13]->nn_param.conv2d.pad[3] = 1;
    node[13]->nn_param.conv2d.group = 1;
    node[13]->nn_param.conv2d.dilation[0] = 1;
    node[13]->nn_param.conv2d.dilation[1] = 1;
    node[13]->vx_param.has_relu = TRUE;
    node[13]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[13]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[13]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_1/sep_57_relu5_1/sep_60
      var       - node[14]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_CONV_RELU, 3, 1, 60);
    node[14]->nn_param.conv2d.ksize[0] = 1;
    node[14]->nn_param.conv2d.ksize[1] = 1;
    node[14]->nn_param.conv2d.weights = 512;
    node[14]->nn_param.conv2d.stride[0] = 1;
    node[14]->nn_param.conv2d.stride[1] = 1;
    node[14]->nn_param.conv2d.pad[0] = 0;
    node[14]->nn_param.conv2d.pad[1] = 0;
    node[14]->nn_param.conv2d.pad[2] = 0;
    node[14]->nn_param.conv2d.pad[3] = 0;
    node[14]->nn_param.conv2d.group = 1;
    node[14]->nn_param.conv2d.dilation[0] = 1;
    node[14]->nn_param.conv2d.dilation[1] = 1;
    node[14]->vx_param.has_relu = TRUE;
    node[14]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[14]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[14]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_2/dw_61_relu5_2/dw_64
      var       - node[15]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_CONV_RELU, 3, 1, 64);
    node[15]->nn_param.conv2d.ksize[0] = 3;
    node[15]->nn_param.conv2d.ksize[1] = 3;
    node[15]->nn_param.conv2d.weights = 512;
    node[15]->nn_param.conv2d.stride[0] = 1;
    node[15]->nn_param.conv2d.stride[1] = 1;
    node[15]->nn_param.conv2d.pad[0] = 1;
    node[15]->nn_param.conv2d.pad[1] = 1;
    node[15]->nn_param.conv2d.pad[2] = 1;
    node[15]->nn_param.conv2d.pad[3] = 1;
    node[15]->nn_param.conv2d.group = 1;
    node[15]->nn_param.conv2d.dilation[0] = 1;
    node[15]->nn_param.conv2d.dilation[1] = 1;
    node[15]->vx_param.has_relu = TRUE;
    node[15]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[15]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[15]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_2/sep_65_relu5_2/sep_68
      var       - node[16]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_CONV_RELU, 3, 1, 68);
    node[16]->nn_param.conv2d.ksize[0] = 1;
    node[16]->nn_param.conv2d.ksize[1] = 1;
    node[16]->nn_param.conv2d.weights = 512;
    node[16]->nn_param.conv2d.stride[0] = 1;
    node[16]->nn_param.conv2d.stride[1] = 1;
    node[16]->nn_param.conv2d.pad[0] = 0;
    node[16]->nn_param.conv2d.pad[1] = 0;
    node[16]->nn_param.conv2d.pad[2] = 0;
    node[16]->nn_param.conv2d.pad[3] = 0;
    node[16]->nn_param.conv2d.group = 1;
    node[16]->nn_param.conv2d.dilation[0] = 1;
    node[16]->nn_param.conv2d.dilation[1] = 1;
    node[16]->vx_param.has_relu = TRUE;
    node[16]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[16]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[16]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_3/dw_69_relu5_3/dw_72
      var       - node[17]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_CONV_RELU, 3, 1, 72);
    node[17]->nn_param.conv2d.ksize[0] = 3;
    node[17]->nn_param.conv2d.ksize[1] = 3;
    node[17]->nn_param.conv2d.weights = 512;
    node[17]->nn_param.conv2d.stride[0] = 1;
    node[17]->nn_param.conv2d.stride[1] = 1;
    node[17]->nn_param.conv2d.pad[0] = 1;
    node[17]->nn_param.conv2d.pad[1] = 1;
    node[17]->nn_param.conv2d.pad[2] = 1;
    node[17]->nn_param.conv2d.pad[3] = 1;
    node[17]->nn_param.conv2d.group = 1;
    node[17]->nn_param.conv2d.dilation[0] = 1;
    node[17]->nn_param.conv2d.dilation[1] = 1;
    node[17]->vx_param.has_relu = TRUE;
    node[17]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[17]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[17]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_3/sep_73_relu5_3/sep_76
      var       - node[18]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_CONV_RELU, 3, 1, 76);
    node[18]->nn_param.conv2d.ksize[0] = 1;
    node[18]->nn_param.conv2d.ksize[1] = 1;
    node[18]->nn_param.conv2d.weights = 512;
    node[18]->nn_param.conv2d.stride[0] = 1;
    node[18]->nn_param.conv2d.stride[1] = 1;
    node[18]->nn_param.conv2d.pad[0] = 0;
    node[18]->nn_param.conv2d.pad[1] = 0;
    node[18]->nn_param.conv2d.pad[2] = 0;
    node[18]->nn_param.conv2d.pad[3] = 0;
    node[18]->nn_param.conv2d.group = 1;
    node[18]->nn_param.conv2d.dilation[0] = 1;
    node[18]->nn_param.conv2d.dilation[1] = 1;
    node[18]->vx_param.has_relu = TRUE;
    node[18]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[18]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[18]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_4/dw_77_relu5_4/dw_80
      var       - node[19]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_CONV_RELU, 3, 1, 80);
    node[19]->nn_param.conv2d.ksize[0] = 3;
    node[19]->nn_param.conv2d.ksize[1] = 3;
    node[19]->nn_param.conv2d.weights = 512;
    node[19]->nn_param.conv2d.stride[0] = 1;
    node[19]->nn_param.conv2d.stride[1] = 1;
    node[19]->nn_param.conv2d.pad[0] = 1;
    node[19]->nn_param.conv2d.pad[1] = 1;
    node[19]->nn_param.conv2d.pad[2] = 1;
    node[19]->nn_param.conv2d.pad[3] = 1;
    node[19]->nn_param.conv2d.group = 1;
    node[19]->nn_param.conv2d.dilation[0] = 1;
    node[19]->nn_param.conv2d.dilation[1] = 1;
    node[19]->vx_param.has_relu = TRUE;
    node[19]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[19]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[19]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_4/sep_81_relu5_4/sep_84
      var       - node[20]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_CONV_RELU, 3, 1, 84);
    node[20]->nn_param.conv2d.ksize[0] = 1;
    node[20]->nn_param.conv2d.ksize[1] = 1;
    node[20]->nn_param.conv2d.weights = 512;
    node[20]->nn_param.conv2d.stride[0] = 1;
    node[20]->nn_param.conv2d.stride[1] = 1;
    node[20]->nn_param.conv2d.pad[0] = 0;
    node[20]->nn_param.conv2d.pad[1] = 0;
    node[20]->nn_param.conv2d.pad[2] = 0;
    node[20]->nn_param.conv2d.pad[3] = 0;
    node[20]->nn_param.conv2d.group = 1;
    node[20]->nn_param.conv2d.dilation[0] = 1;
    node[20]->nn_param.conv2d.dilation[1] = 1;
    node[20]->vx_param.has_relu = TRUE;
    node[20]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[20]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[20]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_5/dw_85_relu5_5/dw_88
      var       - node[21]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_CONV_RELU, 3, 1, 88);
    node[21]->nn_param.conv2d.ksize[0] = 3;
    node[21]->nn_param.conv2d.ksize[1] = 3;
    node[21]->nn_param.conv2d.weights = 512;
    node[21]->nn_param.conv2d.stride[0] = 1;
    node[21]->nn_param.conv2d.stride[1] = 1;
    node[21]->nn_param.conv2d.pad[0] = 1;
    node[21]->nn_param.conv2d.pad[1] = 1;
    node[21]->nn_param.conv2d.pad[2] = 1;
    node[21]->nn_param.conv2d.pad[3] = 1;
    node[21]->nn_param.conv2d.group = 1;
    node[21]->nn_param.conv2d.dilation[0] = 1;
    node[21]->nn_param.conv2d.dilation[1] = 1;
    node[21]->vx_param.has_relu = TRUE;
    node[21]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[21]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[21]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_5/sep_89_relu5_5/sep_92
      var       - node[22]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[22], VSI_NN_OP_CONV_RELU, 3, 1, 92);
    node[22]->nn_param.conv2d.ksize[0] = 1;
    node[22]->nn_param.conv2d.ksize[1] = 1;
    node[22]->nn_param.conv2d.weights = 512;
    node[22]->nn_param.conv2d.stride[0] = 1;
    node[22]->nn_param.conv2d.stride[1] = 1;
    node[22]->nn_param.conv2d.pad[0] = 0;
    node[22]->nn_param.conv2d.pad[1] = 0;
    node[22]->nn_param.conv2d.pad[2] = 0;
    node[22]->nn_param.conv2d.pad[3] = 0;
    node[22]->nn_param.conv2d.group = 1;
    node[22]->nn_param.conv2d.dilation[0] = 1;
    node[22]->nn_param.conv2d.dilation[1] = 1;
    node[22]->vx_param.has_relu = TRUE;
    node[22]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[22]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[22]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_6/dw_93_relu5_6/dw_96
      var       - node[23]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[7, 7, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[23], VSI_NN_OP_CONV_RELU, 3, 1, 96);
    node[23]->nn_param.conv2d.ksize[0] = 3;
    node[23]->nn_param.conv2d.ksize[1] = 3;
    node[23]->nn_param.conv2d.weights = 512;
    node[23]->nn_param.conv2d.stride[0] = 2;
    node[23]->nn_param.conv2d.stride[1] = 2;
    node[23]->nn_param.conv2d.pad[0] = 1;
    node[23]->nn_param.conv2d.pad[1] = 1;
    node[23]->nn_param.conv2d.pad[2] = 1;
    node[23]->nn_param.conv2d.pad[3] = 1;
    node[23]->nn_param.conv2d.group = 1;
    node[23]->nn_param.conv2d.dilation[0] = 1;
    node[23]->nn_param.conv2d.dilation[1] = 1;
    node[23]->vx_param.has_relu = TRUE;
    node[23]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[23]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[23]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv5_6/sep_97_relu5_6/sep_100
      var       - node[24]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 512, 1]]
      out_shape - [[7, 7, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[24], VSI_NN_OP_CONV_RELU, 3, 1, 100);
    node[24]->nn_param.conv2d.ksize[0] = 1;
    node[24]->nn_param.conv2d.ksize[1] = 1;
    node[24]->nn_param.conv2d.weights = 1024;
    node[24]->nn_param.conv2d.stride[0] = 1;
    node[24]->nn_param.conv2d.stride[1] = 1;
    node[24]->nn_param.conv2d.pad[0] = 0;
    node[24]->nn_param.conv2d.pad[1] = 0;
    node[24]->nn_param.conv2d.pad[2] = 0;
    node[24]->nn_param.conv2d.pad[3] = 0;
    node[24]->nn_param.conv2d.group = 1;
    node[24]->nn_param.conv2d.dilation[0] = 1;
    node[24]->nn_param.conv2d.dilation[1] = 1;
    node[24]->vx_param.has_relu = TRUE;
    node[24]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[24]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[24]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv6/dw_101_relu6/dw_104
      var       - node[25]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 1024, 1]]
      out_shape - [[7, 7, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[25], VSI_NN_OP_CONV_RELU, 3, 1, 104);
    node[25]->nn_param.conv2d.ksize[0] = 3;
    node[25]->nn_param.conv2d.ksize[1] = 3;
    node[25]->nn_param.conv2d.weights = 1024;
    node[25]->nn_param.conv2d.stride[0] = 1;
    node[25]->nn_param.conv2d.stride[1] = 1;
    node[25]->nn_param.conv2d.pad[0] = 1;
    node[25]->nn_param.conv2d.pad[1] = 1;
    node[25]->nn_param.conv2d.pad[2] = 1;
    node[25]->nn_param.conv2d.pad[3] = 1;
    node[25]->nn_param.conv2d.group = 1;
    node[25]->nn_param.conv2d.dilation[0] = 1;
    node[25]->nn_param.conv2d.dilation[1] = 1;
    node[25]->vx_param.has_relu = TRUE;
    node[25]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[25]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[25]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv6/sep_105_relu6/sep_108
      var       - node[26]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 1024, 1]]
      out_shape - [[7, 7, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[26], VSI_NN_OP_CONV_RELU, 3, 1, 108);
    node[26]->nn_param.conv2d.ksize[0] = 1;
    node[26]->nn_param.conv2d.ksize[1] = 1;
    node[26]->nn_param.conv2d.weights = 1024;
    node[26]->nn_param.conv2d.stride[0] = 1;
    node[26]->nn_param.conv2d.stride[1] = 1;
    node[26]->nn_param.conv2d.pad[0] = 0;
    node[26]->nn_param.conv2d.pad[1] = 0;
    node[26]->nn_param.conv2d.pad[2] = 0;
    node[26]->nn_param.conv2d.pad[3] = 0;
    node[26]->nn_param.conv2d.group = 1;
    node[26]->nn_param.conv2d.dilation[0] = 1;
    node[26]->nn_param.conv2d.dilation[1] = 1;
    node[26]->vx_param.has_relu = TRUE;
    node[26]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[26]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[26]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - pool6_109
      var       - node[27]
      name      - pool6
      operation - pooling
      in_shape  - [[7, 7, 1024, 1]]
      out_shape - [[1, 1, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[27], VSI_NN_OP_POOL, 1, 1, 109);
    node[27]->nn_param.pool.ksize[0] = 7;
    node[27]->nn_param.pool.ksize[1] = 7;
    node[27]->nn_param.pool.stride[0] = 1;
    node[27]->nn_param.pool.stride[1] = 1;
    node[27]->nn_param.pool.pad[0] = 0;
    node[27]->nn_param.pool.pad[1] = 0;
    node[27]->nn_param.pool.pad[2] = 0;
    node[27]->nn_param.pool.pad[3] = 0;
    node[27]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_AVG;
    node[27]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[27]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - trans_fc7_110
      var       - node[28]
      name      - fullconnectrelu
      operation - fullconnectrelu
      in_shape  - [[1, 1, 1024, 1]]
      out_shape - [[1000, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[28], VSI_NN_OP_FCL_RELU, 3, 1, 110);
    node[28]->nn_param.fcl.weights = 1000;
    node[28]->vx_param.has_relu = FALSE;
    node[28]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[28]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[28]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - fc7_110_reshape_2
      var       - node[29]
      name      - fc7_110_reshape
      operation - reshape
      in_shape  - [[1000, 1]]
      out_shape - [[1, 1, 1000, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[29], VSI_NN_OP_RESHAPE, 1, 1, 2);
    node[29]->nn_param.reshape.size = shape_1;
    node[29]->nn_param.reshape.dim_num = 4;


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @data_0:out0 */
    attr.size[0] = 224;
    attr.size[1] = 224;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_INT8);

    /* @output_111:out0 */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 1000;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_INT8);



    /* @conv1_1_relu1_4:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 3;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_INT8, 128, 864);

    /* @conv1_1_relu1_4:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_INT32, 0, 128);

    /* @conv2_1/dw_5_relu2_1/dw_8:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 32;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_INT8, 1120, 9216);

    /* @conv2_1/dw_5_relu2_1/dw_8:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_INT32, 992, 128);

    /* @conv2_1/sep_9_relu2_1/sep_12:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 32;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_INT8, 10592, 2048);

    /* @conv2_1/sep_9_relu2_1/sep_12:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_INT32, 10336, 256);

    /* @conv2_2/dw_13_relu2_2/dw_16:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 64;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_INT8, 12896, 36864);

    /* @conv2_2/dw_13_relu2_2/dw_16:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_INT32, 12640, 256);

    /* @conv2_2/sep_17_relu2_2/sep_20:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 64;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_INT8, 50272, 8192);

    /* @conv2_2/sep_17_relu2_2/sep_20:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_INT32, 49760, 512);

    /* @conv3_1/dw_21_relu3_1/dw_24:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 128;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_INT8, 58976, 147456);

    /* @conv3_1/dw_21_relu3_1/dw_24:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_INT32, 58464, 512);

    /* @conv3_1/sep_25_relu3_1/sep_28:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 128;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_INT8, 206944, 16384);

    /* @conv3_1/sep_25_relu3_1/sep_28:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_INT32, 206432, 512);

    /* @conv3_2/dw_29_relu3_2/dw_32:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 128;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_INT8, 223840, 147456);

    /* @conv3_2/dw_29_relu3_2/dw_32:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_INT32, 223328, 512);

    /* @conv3_2/sep_33_relu3_2/sep_36:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 128;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_INT8, 372320, 32768);

    /* @conv3_2/sep_33_relu3_2/sep_36:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_INT32, 371296, 1024);

    /* @conv4_1/dw_37_relu4_1/dw_40:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 256;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_INT8, 406112, 589824);

    /* @conv4_1/dw_37_relu4_1/dw_40:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[19], attr, VSI_NN_TYPE_INT32, 405088, 1024);

    /* @conv4_1/sep_41_relu4_1/sep_44:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[20], attr, VSI_NN_TYPE_INT8, 996960, 65536);

    /* @conv4_1/sep_41_relu4_1/sep_44:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[21], attr, VSI_NN_TYPE_INT32, 995936, 1024);

    /* @conv4_2/dw_45_relu4_2/dw_48:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 256;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[22], attr, VSI_NN_TYPE_INT8, 1063520, 589824);

    /* @conv4_2/dw_45_relu4_2/dw_48:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[23], attr, VSI_NN_TYPE_INT32, 1062496, 1024);

    /* @conv4_2/sep_49_relu4_2/sep_52:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[24], attr, VSI_NN_TYPE_INT8, 1655392, 131072);

    /* @conv4_2/sep_49_relu4_2/sep_52:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[25], attr, VSI_NN_TYPE_INT32, 1653344, 2048);

    /* @conv5_1/dw_53_relu5_1/dw_56:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[26], attr, VSI_NN_TYPE_INT8, 1788512, 2359296);

    /* @conv5_1/dw_53_relu5_1/dw_56:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[27], attr, VSI_NN_TYPE_INT32, 1786464, 2048);

    /* @conv5_1/sep_57_relu5_1/sep_60:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[28], attr, VSI_NN_TYPE_INT8, 4149856, 262144);

    /* @conv5_1/sep_57_relu5_1/sep_60:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[29], attr, VSI_NN_TYPE_INT32, 4147808, 2048);

    /* @conv5_2/dw_61_relu5_2/dw_64:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[30], attr, VSI_NN_TYPE_INT8, 4414048, 2359296);

    /* @conv5_2/dw_61_relu5_2/dw_64:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[31], attr, VSI_NN_TYPE_INT32, 4412000, 2048);

    /* @conv5_2/sep_65_relu5_2/sep_68:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[32], attr, VSI_NN_TYPE_INT8, 6775392, 262144);

    /* @conv5_2/sep_65_relu5_2/sep_68:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 11;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[33], attr, VSI_NN_TYPE_INT32, 6773344, 2048);

    /* @conv5_3/dw_69_relu5_3/dw_72:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[34], attr, VSI_NN_TYPE_INT8, 7039584, 2359296);

    /* @conv5_3/dw_69_relu5_3/dw_72:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[35], attr, VSI_NN_TYPE_INT32, 7037536, 2048);

    /* @conv5_3/sep_73_relu5_3/sep_76:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[36], attr, VSI_NN_TYPE_INT8, 9400928, 262144);

    /* @conv5_3/sep_73_relu5_3/sep_76:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[37], attr, VSI_NN_TYPE_INT32, 9398880, 2048);

    /* @conv5_4/dw_77_relu5_4/dw_80:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[38], attr, VSI_NN_TYPE_INT8, 9665120, 2359296);

    /* @conv5_4/dw_77_relu5_4/dw_80:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[39], attr, VSI_NN_TYPE_INT32, 9663072, 2048);

    /* @conv5_4/sep_81_relu5_4/sep_84:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[40], attr, VSI_NN_TYPE_INT8, 12026464, 262144);

    /* @conv5_4/sep_81_relu5_4/sep_84:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[41], attr, VSI_NN_TYPE_INT32, 12024416, 2048);

    /* @conv5_5/dw_85_relu5_5/dw_88:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[42], attr, VSI_NN_TYPE_INT8, 12290656, 2359296);

    /* @conv5_5/dw_85_relu5_5/dw_88:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[43], attr, VSI_NN_TYPE_INT32, 12288608, 2048);

    /* @conv5_5/sep_89_relu5_5/sep_92:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[44], attr, VSI_NN_TYPE_INT8, 14652000, 262144);

    /* @conv5_5/sep_89_relu5_5/sep_92:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[45], attr, VSI_NN_TYPE_INT32, 14649952, 2048);

    /* @conv5_6/dw_93_relu5_6/dw_96:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 512;
    attr.size[3] = 512;
    attr.dim_num = 4;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[46], attr, VSI_NN_TYPE_INT8, 14916192, 2359296);

    /* @conv5_6/dw_93_relu5_6/dw_96:bias */
    attr.size[0] = 512;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[47], attr, VSI_NN_TYPE_INT32, 14914144, 2048);

    /* @conv5_6/sep_97_relu5_6/sep_100:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 1024;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[48], attr, VSI_NN_TYPE_INT8, 17279584, 524288);

    /* @conv5_6/sep_97_relu5_6/sep_100:bias */
    attr.size[0] = 1024;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[49], attr, VSI_NN_TYPE_INT32, 17275488, 4096);

    /* @conv6/dw_101_relu6/dw_104:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 1024;
    attr.size[3] = 1024;
    attr.dim_num = 4;
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[50], attr, VSI_NN_TYPE_INT8, 17807968, 9437184);

    /* @conv6/dw_101_relu6/dw_104:bias */
    attr.size[0] = 1024;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[51], attr, VSI_NN_TYPE_INT32, 17803872, 4096);

    /* @conv6/sep_105_relu6/sep_108:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 1024;
    attr.size[3] = 1024;
    attr.dim_num = 4;
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[52], attr, VSI_NN_TYPE_INT8, 27249248, 1048576);

    /* @conv6/sep_105_relu6/sep_108:bias */
    attr.size[0] = 1024;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[53], attr, VSI_NN_TYPE_INT32, 27245152, 4096);

    /* @trans_fc7_110:weight */
    attr.size[0] = 1024;
    attr.size[1] = 1000;
    attr.dim_num = 2;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[54], attr, VSI_NN_TYPE_INT8, 28301824, 1024000);

    /* @trans_fc7_110:bias */
    attr.size[0] = 1000;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[55], attr, VSI_NN_TYPE_INT32, 28297824, 4000);



    /* @conv1_1_relu1_4:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2_1/dw_5_relu2_1/dw_8:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2_1/sep_9_relu2_1/sep_12:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2_2/dw_13_relu2_2/dw_16:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2_2/sep_17_relu2_2/sep_20:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv3_1/dw_21_relu3_1/dw_24:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv3_1/sep_25_relu3_1/sep_28:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv3_2/dw_29_relu3_2/dw_32:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv3_2/sep_33_relu3_2/sep_36:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv4_1/dw_37_relu4_1/dw_40:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv4_1/sep_41_relu4_1/sep_44:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv4_2/dw_45_relu4_2/dw_48:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv4_2/sep_49_relu4_2/sep_52:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_1/dw_53_relu5_1/dw_56:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_1/sep_57_relu5_1/sep_60:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_2/dw_61_relu5_2/dw_64:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_2/sep_65_relu5_2/sep_68:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_3/dw_69_relu5_3/dw_72:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_3/sep_73_relu5_3/sep_76:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_4/dw_77_relu5_4/dw_80:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_4/sep_81_relu5_4/sep_84:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_5/dw_85_relu5_5/dw_88:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[21]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_5/sep_89_relu5_5/sep_92:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[22]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_6/dw_93_relu5_6/dw_96:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[23]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv5_6/sep_97_relu5_6/sep_100:out0 */
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[24]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv6/dw_101_relu6/dw_104:out0 */
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[25]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv6/sep_105_relu6/sep_108:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[26]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool6_109:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[27]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_fc7_110:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[28]->output.tensors[0], attr, VSI_NN_TYPE_INT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[0]->input.tensors[0] = norm_tensor[0];
    node[29]->output.tensors[0] = norm_tensor[1];

    /* conv1_1_relu1_4 */
    node[0]->input.tensors[1] = const_tensor[0]; /* data_weight */
    node[0]->input.tensors[2] = const_tensor[1]; /* data_bias */

    /* conv2_1/dw_5_relu2_1/dw_8 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];
    node[1]->input.tensors[1] = const_tensor[2]; /* data_weight */
    node[1]->input.tensors[2] = const_tensor[3]; /* data_bias */

    /* conv2_1/sep_9_relu2_1/sep_12 */
    node[2]->input.tensors[0] = node[1]->output.tensors[0];
    node[2]->input.tensors[1] = const_tensor[4]; /* data_weight */
    node[2]->input.tensors[2] = const_tensor[5]; /* data_bias */

    /* conv2_2/dw_13_relu2_2/dw_16 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];
    node[3]->input.tensors[1] = const_tensor[6]; /* data_weight */
    node[3]->input.tensors[2] = const_tensor[7]; /* data_bias */

    /* conv2_2/sep_17_relu2_2/sep_20 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[8]; /* data_weight */
    node[4]->input.tensors[2] = const_tensor[9]; /* data_bias */

    /* conv3_1/dw_21_relu3_1/dw_24 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];
    node[5]->input.tensors[1] = const_tensor[10]; /* data_weight */
    node[5]->input.tensors[2] = const_tensor[11]; /* data_bias */

    /* conv3_1/sep_25_relu3_1/sep_28 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];
    node[6]->input.tensors[1] = const_tensor[12]; /* data_weight */
    node[6]->input.tensors[2] = const_tensor[13]; /* data_bias */

    /* conv3_2/dw_29_relu3_2/dw_32 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];
    node[7]->input.tensors[1] = const_tensor[14]; /* data_weight */
    node[7]->input.tensors[2] = const_tensor[15]; /* data_bias */

    /* conv3_2/sep_33_relu3_2/sep_36 */
    node[8]->input.tensors[0] = node[7]->output.tensors[0];
    node[8]->input.tensors[1] = const_tensor[16]; /* data_weight */
    node[8]->input.tensors[2] = const_tensor[17]; /* data_bias */

    /* conv4_1/dw_37_relu4_1/dw_40 */
    node[9]->input.tensors[0] = node[8]->output.tensors[0];
    node[9]->input.tensors[1] = const_tensor[18]; /* data_weight */
    node[9]->input.tensors[2] = const_tensor[19]; /* data_bias */

    /* conv4_1/sep_41_relu4_1/sep_44 */
    node[10]->input.tensors[0] = node[9]->output.tensors[0];
    node[10]->input.tensors[1] = const_tensor[20]; /* data_weight */
    node[10]->input.tensors[2] = const_tensor[21]; /* data_bias */

    /* conv4_2/dw_45_relu4_2/dw_48 */
    node[11]->input.tensors[0] = node[10]->output.tensors[0];
    node[11]->input.tensors[1] = const_tensor[22]; /* data_weight */
    node[11]->input.tensors[2] = const_tensor[23]; /* data_bias */

    /* conv4_2/sep_49_relu4_2/sep_52 */
    node[12]->input.tensors[0] = node[11]->output.tensors[0];
    node[12]->input.tensors[1] = const_tensor[24]; /* data_weight */
    node[12]->input.tensors[2] = const_tensor[25]; /* data_bias */

    /* conv5_1/dw_53_relu5_1/dw_56 */
    node[13]->input.tensors[0] = node[12]->output.tensors[0];
    node[13]->input.tensors[1] = const_tensor[26]; /* data_weight */
    node[13]->input.tensors[2] = const_tensor[27]; /* data_bias */

    /* conv5_1/sep_57_relu5_1/sep_60 */
    node[14]->input.tensors[0] = node[13]->output.tensors[0];
    node[14]->input.tensors[1] = const_tensor[28]; /* data_weight */
    node[14]->input.tensors[2] = const_tensor[29]; /* data_bias */

    /* conv5_2/dw_61_relu5_2/dw_64 */
    node[15]->input.tensors[0] = node[14]->output.tensors[0];
    node[15]->input.tensors[1] = const_tensor[30]; /* data_weight */
    node[15]->input.tensors[2] = const_tensor[31]; /* data_bias */

    /* conv5_2/sep_65_relu5_2/sep_68 */
    node[16]->input.tensors[0] = node[15]->output.tensors[0];
    node[16]->input.tensors[1] = const_tensor[32]; /* data_weight */
    node[16]->input.tensors[2] = const_tensor[33]; /* data_bias */

    /* conv5_3/dw_69_relu5_3/dw_72 */
    node[17]->input.tensors[0] = node[16]->output.tensors[0];
    node[17]->input.tensors[1] = const_tensor[34]; /* data_weight */
    node[17]->input.tensors[2] = const_tensor[35]; /* data_bias */

    /* conv5_3/sep_73_relu5_3/sep_76 */
    node[18]->input.tensors[0] = node[17]->output.tensors[0];
    node[18]->input.tensors[1] = const_tensor[36]; /* data_weight */
    node[18]->input.tensors[2] = const_tensor[37]; /* data_bias */

    /* conv5_4/dw_77_relu5_4/dw_80 */
    node[19]->input.tensors[0] = node[18]->output.tensors[0];
    node[19]->input.tensors[1] = const_tensor[38]; /* data_weight */
    node[19]->input.tensors[2] = const_tensor[39]; /* data_bias */

    /* conv5_4/sep_81_relu5_4/sep_84 */
    node[20]->input.tensors[0] = node[19]->output.tensors[0];
    node[20]->input.tensors[1] = const_tensor[40]; /* data_weight */
    node[20]->input.tensors[2] = const_tensor[41]; /* data_bias */

    /* conv5_5/dw_85_relu5_5/dw_88 */
    node[21]->input.tensors[0] = node[20]->output.tensors[0];
    node[21]->input.tensors[1] = const_tensor[42]; /* data_weight */
    node[21]->input.tensors[2] = const_tensor[43]; /* data_bias */

    /* conv5_5/sep_89_relu5_5/sep_92 */
    node[22]->input.tensors[0] = node[21]->output.tensors[0];
    node[22]->input.tensors[1] = const_tensor[44]; /* data_weight */
    node[22]->input.tensors[2] = const_tensor[45]; /* data_bias */

    /* conv5_6/dw_93_relu5_6/dw_96 */
    node[23]->input.tensors[0] = node[22]->output.tensors[0];
    node[23]->input.tensors[1] = const_tensor[46]; /* data_weight */
    node[23]->input.tensors[2] = const_tensor[47]; /* data_bias */

    /* conv5_6/sep_97_relu5_6/sep_100 */
    node[24]->input.tensors[0] = node[23]->output.tensors[0];
    node[24]->input.tensors[1] = const_tensor[48]; /* data_weight */
    node[24]->input.tensors[2] = const_tensor[49]; /* data_bias */

    /* conv6/dw_101_relu6/dw_104 */
    node[25]->input.tensors[0] = node[24]->output.tensors[0];
    node[25]->input.tensors[1] = const_tensor[50]; /* data_weight */
    node[25]->input.tensors[2] = const_tensor[51]; /* data_bias */

    /* conv6/sep_105_relu6/sep_108 */
    node[26]->input.tensors[0] = node[25]->output.tensors[0];
    node[26]->input.tensors[1] = const_tensor[52]; /* data_weight */
    node[26]->input.tensors[2] = const_tensor[53]; /* data_bias */

    /* pool6_109 */
    node[27]->input.tensors[0] = node[26]->output.tensors[0];

    /* trans_fc7_110 */
    node[28]->input.tensors[0] = node[27]->output.tensors[0];
    node[28]->input.tensors[1] = const_tensor[54]; /* data_weight */
    node[28]->input.tensors[2] = const_tensor[55]; /* data_bias */

    /* fc7_110_reshape_2 */
    node[29]->input.tensors[0] = node[28]->output.tensors[0];



    graph->input.tensors[0] = norm_tensor[0];
    graph->output.tensors[0] = norm_tensor[1];


    status = vsi_nn_SetupGraph( graph, FALSE );
    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vnn_ReleaseMobilenet( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateMobilenet() */

void vnn_ReleaseMobilenet
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseMobilenet() */

