/****************************************************************************
*   Generated by ACUITY 3.11.0
*   Match ovxlib 1.0.8
*
*   Neural Network appliction network definition source file
****************************************************************************/
/*-------------------------------------------
                   Includes
 -------------------------------------------*/
#include <stdio.h>
#include <stdlib.h>

#include "vsi_nn_pub.h"

#include "vnn_global.h"
#include "vnn_bvlcgooglenetcaffe.h"

/*-------------------------------------------
                   Macros
 -------------------------------------------*/

#define NEW_VXNODE(_node, _type, _in, _out, _uid) do {\
        _node = vsi_nn_AddNode( graph, _type, _in, _out, NULL );\
        _node->uid = (uint32_t)_uid; \
        if( NULL == _node ) {\
            goto error;\
        }\
    } while(0)

#define NEW_VIRTUAL_TENSOR(_id, _attr, _dtype) do {\
        memset( _attr.size, 0, VSI_NN_MAX_DIM_NUM * sizeof(uint32_t));\
        _attr.dim_num = VSI_NN_DIM_AUTO;\
        _attr.vtl = !VNN_APP_DEBUG;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set const tensor dims out of this macro.
#define NEW_CONST_TENSOR(_id, _attr, _dtype, _ofst, _size) do {\
        data = load_data( fp, _ofst, _size  );\
        _attr.vtl = FALSE;\
        _attr.is_const = TRUE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, data );\
        free( data );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

// Set generic tensor dims out of this macro.
#define NEW_NORM_TENSOR(_id, _attr, _dtype) do {\
        _attr.vtl = FALSE;\
        _attr.is_const = FALSE;\
        _attr.dtype.vx_type = _dtype;\
        _id = vsi_nn_AddTensor( graph, VSI_NN_TENSOR_ID_AUTO,\
                & _attr, NULL );\
        if( VSI_NN_TENSOR_ID_NA == _id ) {\
            goto error;\
        }\
    } while(0)

#define NET_NODE_NUM            (84)
#define NET_NORM_TENSOR_NUM     (2)
#define NET_CONST_TENSOR_NUM    (116)
#define NET_VIRTUAL_TENSOR_NUM  (84)
#define NET_TOTAL_TENSOR_NUM    (NET_NORM_TENSOR_NUM + NET_CONST_TENSOR_NUM + NET_VIRTUAL_TENSOR_NUM + 32)

/*-------------------------------------------
               Local Variables
 -------------------------------------------*/

/*-------------------------------------------
                  Functions
 -------------------------------------------*/
static void load_hw_config
    (
    vsi_nn_context_t ctx
    )
{
    ctx->config.evis.ver = VSI_NN_HW_EVIS_2;
    strncpy(ctx->config.target_name, "VIP8000", VSI_NN_MAX_TARGET_NAME);
}

static uint8_t* load_data
    (
    FILE  * fp,
    size_t  ofst,
    size_t  sz
    )
{
    uint8_t* data;
    int32_t ret;
    data = NULL;
    if( NULL == fp )
    {
        return NULL;
    }

    ret = fseek(fp, ofst, SEEK_SET);
    if (ret != 0)
    {
        VSILOGE("blob seek failure.");
        return NULL;
    }

    data = (uint8_t*)malloc(sz);
    if (data == NULL)
    {
        VSILOGE("buffer malloc failure.");
        return NULL;
    }
    ret = fread(data, 1, sz, fp);
    VSILOGI("Read %d data.", ret);
    return data;
} /* load_data() */

vsi_nn_graph_t * vnn_CreateBvlcGooglenetCaffe
    (
    const char * data_file_name,
    vsi_nn_context_t in_ctx
    )
{
    vsi_status              status;
    vsi_bool                release_ctx;
    vsi_nn_context_t        ctx;
    vsi_nn_graph_t *        graph;
    vsi_nn_node_t *         node[NET_NODE_NUM];
    vsi_nn_tensor_id_t      norm_tensor[NET_NORM_TENSOR_NUM];
    vsi_nn_tensor_id_t      const_tensor[NET_CONST_TENSOR_NUM];
    vsi_nn_tensor_attr_t    attr;
    FILE *                  fp;
    uint8_t *               data;



    ctx = NULL;
    graph = NULL;
    status = VSI_FAILURE;

    fp = fopen( data_file_name, "rb" );
    if( NULL == fp )
    {
        VSILOGE( "Open file %s failed.", data_file_name );
        goto error;
    }

    if( NULL == in_ctx )
    {
        ctx = vsi_nn_CreateContext();
        load_hw_config(ctx);
    }
    else
    {
        ctx = in_ctx;
    }

    graph = vsi_nn_CreateGraph( ctx, NET_TOTAL_TENSOR_NUM, NET_NODE_NUM );
    if( NULL == graph )
    {
        VSILOGE( "Create graph fail." );
        goto error;
    }
    vsi_nn_SetGraphInputs( graph, NULL, 1 );
    vsi_nn_SetGraphOutputs( graph, NULL, 1 );

/*-----------------------------------------
  Register client ops
 -----------------------------------------*/


/*-----------------------------------------
  Node definitions
 -----------------------------------------*/

    /*-----------------------------------------
      lid       - conv1/7x7_s2_1_conv1/relu_7x7_2
      var       - node[0]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[224, 224, 3, 1]]
      out_shape - [[112, 112, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[0], VSI_NN_OP_CONV_RELU, 3, 1, 2);
    node[0]->nn_param.conv2d.ksize[0] = 7;
    node[0]->nn_param.conv2d.ksize[1] = 7;
    node[0]->nn_param.conv2d.weights = 64;
    node[0]->nn_param.conv2d.stride[0] = 2;
    node[0]->nn_param.conv2d.stride[1] = 2;
    node[0]->nn_param.conv2d.pad[0] = 3;
    node[0]->nn_param.conv2d.pad[1] = 3;
    node[0]->nn_param.conv2d.pad[2] = 3;
    node[0]->nn_param.conv2d.pad[3] = 3;
    node[0]->nn_param.conv2d.group = 1;
    node[0]->nn_param.conv2d.dilation[0] = 1;
    node[0]->nn_param.conv2d.dilation[1] = 1;
    node[0]->vx_param.has_relu = TRUE;
    node[0]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[0]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[0]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - pool1/3x3_s2_3
      var       - node[1]
      name      - pool1/3x3_s2
      operation - pooling
      in_shape  - [[112, 112, 64, 1]]
      out_shape - [[56, 56, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[1], VSI_NN_OP_POOL, 1, 1, 3);
    node[1]->nn_param.pool.ksize[0] = 3;
    node[1]->nn_param.pool.ksize[1] = 3;
    node[1]->nn_param.pool.stride[0] = 2;
    node[1]->nn_param.pool.stride[1] = 2;
    node[1]->nn_param.pool.pad[0] = 0;
    node[1]->nn_param.pool.pad[1] = 1;
    node[1]->nn_param.pool.pad[2] = 0;
    node[1]->nn_param.pool.pad[3] = 1;
    node[1]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[1]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[1]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - pool1/norm1_4
      var       - node[2]
      name      - pool1/norm1
      operation - localresponsenormalization
      in_shape  - [[56, 56, 64, 1]]
      out_shape - [[56, 56, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[2], VSI_NN_OP_LRN, 1, 1, 4);
    node[2]->nn_param.lrn.type = VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS;
    node[2]->nn_param.lrn.size = 5;
    node[2]->nn_param.lrn.alpha = 0.0001;
    node[2]->nn_param.lrn.beta = 0.75;
    node[2]->nn_param.lrn.bias = 1.0;

    /*-----------------------------------------
      lid       - conv2/3x3_reduce_5_conv2/relu_3x3_reduce_6
      var       - node[3]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 64, 1]]
      out_shape - [[56, 56, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[3], VSI_NN_OP_CONV_RELU, 3, 1, 6);
    node[3]->nn_param.conv2d.ksize[0] = 1;
    node[3]->nn_param.conv2d.ksize[1] = 1;
    node[3]->nn_param.conv2d.weights = 64;
    node[3]->nn_param.conv2d.stride[0] = 1;
    node[3]->nn_param.conv2d.stride[1] = 1;
    node[3]->nn_param.conv2d.pad[0] = 0;
    node[3]->nn_param.conv2d.pad[1] = 0;
    node[3]->nn_param.conv2d.pad[2] = 0;
    node[3]->nn_param.conv2d.pad[3] = 0;
    node[3]->nn_param.conv2d.group = 1;
    node[3]->nn_param.conv2d.dilation[0] = 1;
    node[3]->nn_param.conv2d.dilation[1] = 1;
    node[3]->vx_param.has_relu = TRUE;
    node[3]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[3]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[3]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2/3x3_7_conv2/relu_3x3_8
      var       - node[4]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[56, 56, 64, 1]]
      out_shape - [[56, 56, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[4], VSI_NN_OP_CONV_RELU, 3, 1, 8);
    node[4]->nn_param.conv2d.ksize[0] = 3;
    node[4]->nn_param.conv2d.ksize[1] = 3;
    node[4]->nn_param.conv2d.weights = 192;
    node[4]->nn_param.conv2d.stride[0] = 1;
    node[4]->nn_param.conv2d.stride[1] = 1;
    node[4]->nn_param.conv2d.pad[0] = 1;
    node[4]->nn_param.conv2d.pad[1] = 1;
    node[4]->nn_param.conv2d.pad[2] = 1;
    node[4]->nn_param.conv2d.pad[3] = 1;
    node[4]->nn_param.conv2d.group = 1;
    node[4]->nn_param.conv2d.dilation[0] = 1;
    node[4]->nn_param.conv2d.dilation[1] = 1;
    node[4]->vx_param.has_relu = TRUE;
    node[4]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[4]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[4]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - conv2/norm2_9
      var       - node[5]
      name      - conv2/norm2
      operation - localresponsenormalization
      in_shape  - [[56, 56, 192, 1]]
      out_shape - [[56, 56, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[5], VSI_NN_OP_LRN, 1, 1, 9);
    node[5]->nn_param.lrn.type = VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS;
    node[5]->nn_param.lrn.size = 5;
    node[5]->nn_param.lrn.alpha = 0.0001;
    node[5]->nn_param.lrn.beta = 0.75;
    node[5]->nn_param.lrn.bias = 1.0;

    /*-----------------------------------------
      lid       - pool2/3x3_s2_10
      var       - node[6]
      name      - pool2/3x3_s2
      operation - pooling
      in_shape  - [[56, 56, 192, 1]]
      out_shape - [[28, 28, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[6], VSI_NN_OP_POOL, 1, 1, 10);
    node[6]->nn_param.pool.ksize[0] = 3;
    node[6]->nn_param.pool.ksize[1] = 3;
    node[6]->nn_param.pool.stride[0] = 2;
    node[6]->nn_param.pool.stride[1] = 2;
    node[6]->nn_param.pool.pad[0] = 0;
    node[6]->nn_param.pool.pad[1] = 1;
    node[6]->nn_param.pool.pad[2] = 0;
    node[6]->nn_param.pool.pad[3] = 1;
    node[6]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[6]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[6]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_3a/pool_21
      var       - node[7]
      name      - inception_3a/pool
      operation - pooling
      in_shape  - [[28, 28, 192, 1]]
      out_shape - [[28, 28, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[7], VSI_NN_OP_POOL, 1, 1, 21);
    node[7]->nn_param.pool.ksize[0] = 3;
    node[7]->nn_param.pool.ksize[1] = 3;
    node[7]->nn_param.pool.stride[0] = 1;
    node[7]->nn_param.pool.stride[1] = 1;
    node[7]->nn_param.pool.pad[0] = 1;
    node[7]->nn_param.pool.pad[1] = 1;
    node[7]->nn_param.pool.pad[2] = 1;
    node[7]->nn_param.pool.pad[3] = 1;
    node[7]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[7]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[7]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_3a/1x1_11_inception_3a/relu_1x1_12
      var       - node[8]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 192, 1]]
      out_shape - [[28, 28, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[8], VSI_NN_OP_CONV_RELU, 3, 1, 12);
    node[8]->nn_param.conv2d.ksize[0] = 1;
    node[8]->nn_param.conv2d.ksize[1] = 1;
    node[8]->nn_param.conv2d.weights = 64;
    node[8]->nn_param.conv2d.stride[0] = 1;
    node[8]->nn_param.conv2d.stride[1] = 1;
    node[8]->nn_param.conv2d.pad[0] = 0;
    node[8]->nn_param.conv2d.pad[1] = 0;
    node[8]->nn_param.conv2d.pad[2] = 0;
    node[8]->nn_param.conv2d.pad[3] = 0;
    node[8]->nn_param.conv2d.group = 1;
    node[8]->nn_param.conv2d.dilation[0] = 1;
    node[8]->nn_param.conv2d.dilation[1] = 1;
    node[8]->vx_param.has_relu = TRUE;
    node[8]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[8]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[8]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/3x3_reduce_13_inception_3a/relu_3x3_reduce_14
      var       - node[9]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 192, 1]]
      out_shape - [[28, 28, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[9], VSI_NN_OP_CONV_RELU, 3, 1, 14);
    node[9]->nn_param.conv2d.ksize[0] = 1;
    node[9]->nn_param.conv2d.ksize[1] = 1;
    node[9]->nn_param.conv2d.weights = 96;
    node[9]->nn_param.conv2d.stride[0] = 1;
    node[9]->nn_param.conv2d.stride[1] = 1;
    node[9]->nn_param.conv2d.pad[0] = 0;
    node[9]->nn_param.conv2d.pad[1] = 0;
    node[9]->nn_param.conv2d.pad[2] = 0;
    node[9]->nn_param.conv2d.pad[3] = 0;
    node[9]->nn_param.conv2d.group = 1;
    node[9]->nn_param.conv2d.dilation[0] = 1;
    node[9]->nn_param.conv2d.dilation[1] = 1;
    node[9]->vx_param.has_relu = TRUE;
    node[9]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[9]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[9]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/5x5_reduce_17_inception_3a/relu_5x5_reduce_18
      var       - node[10]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 192, 1]]
      out_shape - [[28, 28, 16, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[10], VSI_NN_OP_CONV_RELU, 3, 1, 18);
    node[10]->nn_param.conv2d.ksize[0] = 1;
    node[10]->nn_param.conv2d.ksize[1] = 1;
    node[10]->nn_param.conv2d.weights = 16;
    node[10]->nn_param.conv2d.stride[0] = 1;
    node[10]->nn_param.conv2d.stride[1] = 1;
    node[10]->nn_param.conv2d.pad[0] = 0;
    node[10]->nn_param.conv2d.pad[1] = 0;
    node[10]->nn_param.conv2d.pad[2] = 0;
    node[10]->nn_param.conv2d.pad[3] = 0;
    node[10]->nn_param.conv2d.group = 1;
    node[10]->nn_param.conv2d.dilation[0] = 1;
    node[10]->nn_param.conv2d.dilation[1] = 1;
    node[10]->vx_param.has_relu = TRUE;
    node[10]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[10]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[10]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/pool_proj_22_inception_3a/relu_pool_proj_23
      var       - node[11]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 192, 1]]
      out_shape - [[28, 28, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[11], VSI_NN_OP_CONV_RELU, 3, 1, 23);
    node[11]->nn_param.conv2d.ksize[0] = 1;
    node[11]->nn_param.conv2d.ksize[1] = 1;
    node[11]->nn_param.conv2d.weights = 32;
    node[11]->nn_param.conv2d.stride[0] = 1;
    node[11]->nn_param.conv2d.stride[1] = 1;
    node[11]->nn_param.conv2d.pad[0] = 0;
    node[11]->nn_param.conv2d.pad[1] = 0;
    node[11]->nn_param.conv2d.pad[2] = 0;
    node[11]->nn_param.conv2d.pad[3] = 0;
    node[11]->nn_param.conv2d.group = 1;
    node[11]->nn_param.conv2d.dilation[0] = 1;
    node[11]->nn_param.conv2d.dilation[1] = 1;
    node[11]->vx_param.has_relu = TRUE;
    node[11]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[11]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[11]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/3x3_15_inception_3a/relu_3x3_16
      var       - node[12]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 96, 1]]
      out_shape - [[28, 28, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[12], VSI_NN_OP_CONV_RELU, 3, 1, 16);
    node[12]->nn_param.conv2d.ksize[0] = 3;
    node[12]->nn_param.conv2d.ksize[1] = 3;
    node[12]->nn_param.conv2d.weights = 128;
    node[12]->nn_param.conv2d.stride[0] = 1;
    node[12]->nn_param.conv2d.stride[1] = 1;
    node[12]->nn_param.conv2d.pad[0] = 1;
    node[12]->nn_param.conv2d.pad[1] = 1;
    node[12]->nn_param.conv2d.pad[2] = 1;
    node[12]->nn_param.conv2d.pad[3] = 1;
    node[12]->nn_param.conv2d.group = 1;
    node[12]->nn_param.conv2d.dilation[0] = 1;
    node[12]->nn_param.conv2d.dilation[1] = 1;
    node[12]->vx_param.has_relu = TRUE;
    node[12]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[12]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[12]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/5x5_19_inception_3a/relu_5x5_20
      var       - node[13]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 16, 1]]
      out_shape - [[28, 28, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[13], VSI_NN_OP_CONV_RELU, 3, 1, 20);
    node[13]->nn_param.conv2d.ksize[0] = 5;
    node[13]->nn_param.conv2d.ksize[1] = 5;
    node[13]->nn_param.conv2d.weights = 32;
    node[13]->nn_param.conv2d.stride[0] = 1;
    node[13]->nn_param.conv2d.stride[1] = 1;
    node[13]->nn_param.conv2d.pad[0] = 2;
    node[13]->nn_param.conv2d.pad[1] = 2;
    node[13]->nn_param.conv2d.pad[2] = 2;
    node[13]->nn_param.conv2d.pad[3] = 2;
    node[13]->nn_param.conv2d.group = 1;
    node[13]->nn_param.conv2d.dilation[0] = 1;
    node[13]->nn_param.conv2d.dilation[1] = 1;
    node[13]->vx_param.has_relu = TRUE;
    node[13]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[13]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[13]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3a/output_24
      var       - node[14]
      name      - inception_3a/output
      operation - concat
      in_shape  - [[28, 28, 64, 1]]
                  [[28, 28, 128, 1]]
                  [[28, 28, 32, 1]]
                  [[28, 28, 32, 1]]
      out_shape - [[28, 28, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[14], VSI_NN_OP_CONCAT, 4, 1, 24);
    node[14]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_3b/pool_35
      var       - node[15]
      name      - inception_3b/pool
      operation - pooling
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[15], VSI_NN_OP_POOL, 1, 1, 35);
    node[15]->nn_param.pool.ksize[0] = 3;
    node[15]->nn_param.pool.ksize[1] = 3;
    node[15]->nn_param.pool.stride[0] = 1;
    node[15]->nn_param.pool.stride[1] = 1;
    node[15]->nn_param.pool.pad[0] = 1;
    node[15]->nn_param.pool.pad[1] = 1;
    node[15]->nn_param.pool.pad[2] = 1;
    node[15]->nn_param.pool.pad[3] = 1;
    node[15]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[15]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[15]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_3b/1x1_25_inception_3b/relu_1x1_26
      var       - node[16]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[16], VSI_NN_OP_CONV_RELU, 3, 1, 26);
    node[16]->nn_param.conv2d.ksize[0] = 1;
    node[16]->nn_param.conv2d.ksize[1] = 1;
    node[16]->nn_param.conv2d.weights = 128;
    node[16]->nn_param.conv2d.stride[0] = 1;
    node[16]->nn_param.conv2d.stride[1] = 1;
    node[16]->nn_param.conv2d.pad[0] = 0;
    node[16]->nn_param.conv2d.pad[1] = 0;
    node[16]->nn_param.conv2d.pad[2] = 0;
    node[16]->nn_param.conv2d.pad[3] = 0;
    node[16]->nn_param.conv2d.group = 1;
    node[16]->nn_param.conv2d.dilation[0] = 1;
    node[16]->nn_param.conv2d.dilation[1] = 1;
    node[16]->vx_param.has_relu = TRUE;
    node[16]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[16]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[16]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/3x3_reduce_27_inception_3b/relu_3x3_reduce_28
      var       - node[17]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[17], VSI_NN_OP_CONV_RELU, 3, 1, 28);
    node[17]->nn_param.conv2d.ksize[0] = 1;
    node[17]->nn_param.conv2d.ksize[1] = 1;
    node[17]->nn_param.conv2d.weights = 128;
    node[17]->nn_param.conv2d.stride[0] = 1;
    node[17]->nn_param.conv2d.stride[1] = 1;
    node[17]->nn_param.conv2d.pad[0] = 0;
    node[17]->nn_param.conv2d.pad[1] = 0;
    node[17]->nn_param.conv2d.pad[2] = 0;
    node[17]->nn_param.conv2d.pad[3] = 0;
    node[17]->nn_param.conv2d.group = 1;
    node[17]->nn_param.conv2d.dilation[0] = 1;
    node[17]->nn_param.conv2d.dilation[1] = 1;
    node[17]->vx_param.has_relu = TRUE;
    node[17]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[17]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[17]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/5x5_reduce_31_inception_3b/relu_5x5_reduce_32
      var       - node[18]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[18], VSI_NN_OP_CONV_RELU, 3, 1, 32);
    node[18]->nn_param.conv2d.ksize[0] = 1;
    node[18]->nn_param.conv2d.ksize[1] = 1;
    node[18]->nn_param.conv2d.weights = 32;
    node[18]->nn_param.conv2d.stride[0] = 1;
    node[18]->nn_param.conv2d.stride[1] = 1;
    node[18]->nn_param.conv2d.pad[0] = 0;
    node[18]->nn_param.conv2d.pad[1] = 0;
    node[18]->nn_param.conv2d.pad[2] = 0;
    node[18]->nn_param.conv2d.pad[3] = 0;
    node[18]->nn_param.conv2d.group = 1;
    node[18]->nn_param.conv2d.dilation[0] = 1;
    node[18]->nn_param.conv2d.dilation[1] = 1;
    node[18]->vx_param.has_relu = TRUE;
    node[18]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[18]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[18]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/pool_proj_36_inception_3b/relu_pool_proj_37
      var       - node[19]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 256, 1]]
      out_shape - [[28, 28, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[19], VSI_NN_OP_CONV_RELU, 3, 1, 37);
    node[19]->nn_param.conv2d.ksize[0] = 1;
    node[19]->nn_param.conv2d.ksize[1] = 1;
    node[19]->nn_param.conv2d.weights = 64;
    node[19]->nn_param.conv2d.stride[0] = 1;
    node[19]->nn_param.conv2d.stride[1] = 1;
    node[19]->nn_param.conv2d.pad[0] = 0;
    node[19]->nn_param.conv2d.pad[1] = 0;
    node[19]->nn_param.conv2d.pad[2] = 0;
    node[19]->nn_param.conv2d.pad[3] = 0;
    node[19]->nn_param.conv2d.group = 1;
    node[19]->nn_param.conv2d.dilation[0] = 1;
    node[19]->nn_param.conv2d.dilation[1] = 1;
    node[19]->vx_param.has_relu = TRUE;
    node[19]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[19]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[19]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/3x3_29_inception_3b/relu_3x3_30
      var       - node[20]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 128, 1]]
      out_shape - [[28, 28, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[20], VSI_NN_OP_CONV_RELU, 3, 1, 30);
    node[20]->nn_param.conv2d.ksize[0] = 3;
    node[20]->nn_param.conv2d.ksize[1] = 3;
    node[20]->nn_param.conv2d.weights = 192;
    node[20]->nn_param.conv2d.stride[0] = 1;
    node[20]->nn_param.conv2d.stride[1] = 1;
    node[20]->nn_param.conv2d.pad[0] = 1;
    node[20]->nn_param.conv2d.pad[1] = 1;
    node[20]->nn_param.conv2d.pad[2] = 1;
    node[20]->nn_param.conv2d.pad[3] = 1;
    node[20]->nn_param.conv2d.group = 1;
    node[20]->nn_param.conv2d.dilation[0] = 1;
    node[20]->nn_param.conv2d.dilation[1] = 1;
    node[20]->vx_param.has_relu = TRUE;
    node[20]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[20]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[20]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/5x5_33_inception_3b/relu_5x5_34
      var       - node[21]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[28, 28, 32, 1]]
      out_shape - [[28, 28, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[21], VSI_NN_OP_CONV_RELU, 3, 1, 34);
    node[21]->nn_param.conv2d.ksize[0] = 5;
    node[21]->nn_param.conv2d.ksize[1] = 5;
    node[21]->nn_param.conv2d.weights = 96;
    node[21]->nn_param.conv2d.stride[0] = 1;
    node[21]->nn_param.conv2d.stride[1] = 1;
    node[21]->nn_param.conv2d.pad[0] = 2;
    node[21]->nn_param.conv2d.pad[1] = 2;
    node[21]->nn_param.conv2d.pad[2] = 2;
    node[21]->nn_param.conv2d.pad[3] = 2;
    node[21]->nn_param.conv2d.group = 1;
    node[21]->nn_param.conv2d.dilation[0] = 1;
    node[21]->nn_param.conv2d.dilation[1] = 1;
    node[21]->vx_param.has_relu = TRUE;
    node[21]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[21]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[21]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_3b/output_38
      var       - node[22]
      name      - inception_3b/output
      operation - concat
      in_shape  - [[28, 28, 128, 1]]
                  [[28, 28, 192, 1]]
                  [[28, 28, 96, 1]]
                  [[28, 28, 64, 1]]
      out_shape - [[28, 28, 480, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[22], VSI_NN_OP_CONCAT, 4, 1, 38);
    node[22]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - pool3/3x3_s2_39
      var       - node[23]
      name      - pool3/3x3_s2
      operation - pooling
      in_shape  - [[28, 28, 480, 1]]
      out_shape - [[14, 14, 480, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[23], VSI_NN_OP_POOL, 1, 1, 39);
    node[23]->nn_param.pool.ksize[0] = 3;
    node[23]->nn_param.pool.ksize[1] = 3;
    node[23]->nn_param.pool.stride[0] = 2;
    node[23]->nn_param.pool.stride[1] = 2;
    node[23]->nn_param.pool.pad[0] = 0;
    node[23]->nn_param.pool.pad[1] = 1;
    node[23]->nn_param.pool.pad[2] = 0;
    node[23]->nn_param.pool.pad[3] = 1;
    node[23]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[23]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[23]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4a/pool_50
      var       - node[24]
      name      - inception_4a/pool
      operation - pooling
      in_shape  - [[14, 14, 480, 1]]
      out_shape - [[14, 14, 480, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[24], VSI_NN_OP_POOL, 1, 1, 50);
    node[24]->nn_param.pool.ksize[0] = 3;
    node[24]->nn_param.pool.ksize[1] = 3;
    node[24]->nn_param.pool.stride[0] = 1;
    node[24]->nn_param.pool.stride[1] = 1;
    node[24]->nn_param.pool.pad[0] = 1;
    node[24]->nn_param.pool.pad[1] = 1;
    node[24]->nn_param.pool.pad[2] = 1;
    node[24]->nn_param.pool.pad[3] = 1;
    node[24]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[24]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[24]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4a/1x1_40_inception_4a/relu_1x1_41
      var       - node[25]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 480, 1]]
      out_shape - [[14, 14, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[25], VSI_NN_OP_CONV_RELU, 3, 1, 41);
    node[25]->nn_param.conv2d.ksize[0] = 1;
    node[25]->nn_param.conv2d.ksize[1] = 1;
    node[25]->nn_param.conv2d.weights = 192;
    node[25]->nn_param.conv2d.stride[0] = 1;
    node[25]->nn_param.conv2d.stride[1] = 1;
    node[25]->nn_param.conv2d.pad[0] = 0;
    node[25]->nn_param.conv2d.pad[1] = 0;
    node[25]->nn_param.conv2d.pad[2] = 0;
    node[25]->nn_param.conv2d.pad[3] = 0;
    node[25]->nn_param.conv2d.group = 1;
    node[25]->nn_param.conv2d.dilation[0] = 1;
    node[25]->nn_param.conv2d.dilation[1] = 1;
    node[25]->vx_param.has_relu = TRUE;
    node[25]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[25]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[25]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/3x3_reduce_42_inception_4a/relu_3x3_reduce_43
      var       - node[26]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 480, 1]]
      out_shape - [[14, 14, 96, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[26], VSI_NN_OP_CONV_RELU, 3, 1, 43);
    node[26]->nn_param.conv2d.ksize[0] = 1;
    node[26]->nn_param.conv2d.ksize[1] = 1;
    node[26]->nn_param.conv2d.weights = 96;
    node[26]->nn_param.conv2d.stride[0] = 1;
    node[26]->nn_param.conv2d.stride[1] = 1;
    node[26]->nn_param.conv2d.pad[0] = 0;
    node[26]->nn_param.conv2d.pad[1] = 0;
    node[26]->nn_param.conv2d.pad[2] = 0;
    node[26]->nn_param.conv2d.pad[3] = 0;
    node[26]->nn_param.conv2d.group = 1;
    node[26]->nn_param.conv2d.dilation[0] = 1;
    node[26]->nn_param.conv2d.dilation[1] = 1;
    node[26]->vx_param.has_relu = TRUE;
    node[26]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[26]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[26]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/5x5_reduce_46_inception_4a/relu_5x5_reduce_47
      var       - node[27]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 480, 1]]
      out_shape - [[14, 14, 16, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[27], VSI_NN_OP_CONV_RELU, 3, 1, 47);
    node[27]->nn_param.conv2d.ksize[0] = 1;
    node[27]->nn_param.conv2d.ksize[1] = 1;
    node[27]->nn_param.conv2d.weights = 16;
    node[27]->nn_param.conv2d.stride[0] = 1;
    node[27]->nn_param.conv2d.stride[1] = 1;
    node[27]->nn_param.conv2d.pad[0] = 0;
    node[27]->nn_param.conv2d.pad[1] = 0;
    node[27]->nn_param.conv2d.pad[2] = 0;
    node[27]->nn_param.conv2d.pad[3] = 0;
    node[27]->nn_param.conv2d.group = 1;
    node[27]->nn_param.conv2d.dilation[0] = 1;
    node[27]->nn_param.conv2d.dilation[1] = 1;
    node[27]->vx_param.has_relu = TRUE;
    node[27]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[27]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[27]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/pool_proj_51_inception_4a/relu_pool_proj_52
      var       - node[28]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 480, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[28], VSI_NN_OP_CONV_RELU, 3, 1, 52);
    node[28]->nn_param.conv2d.ksize[0] = 1;
    node[28]->nn_param.conv2d.ksize[1] = 1;
    node[28]->nn_param.conv2d.weights = 64;
    node[28]->nn_param.conv2d.stride[0] = 1;
    node[28]->nn_param.conv2d.stride[1] = 1;
    node[28]->nn_param.conv2d.pad[0] = 0;
    node[28]->nn_param.conv2d.pad[1] = 0;
    node[28]->nn_param.conv2d.pad[2] = 0;
    node[28]->nn_param.conv2d.pad[3] = 0;
    node[28]->nn_param.conv2d.group = 1;
    node[28]->nn_param.conv2d.dilation[0] = 1;
    node[28]->nn_param.conv2d.dilation[1] = 1;
    node[28]->vx_param.has_relu = TRUE;
    node[28]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[28]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[28]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/3x3_44_inception_4a/relu_3x3_45
      var       - node[29]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 96, 1]]
      out_shape - [[14, 14, 208, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[29], VSI_NN_OP_CONV_RELU, 3, 1, 45);
    node[29]->nn_param.conv2d.ksize[0] = 3;
    node[29]->nn_param.conv2d.ksize[1] = 3;
    node[29]->nn_param.conv2d.weights = 208;
    node[29]->nn_param.conv2d.stride[0] = 1;
    node[29]->nn_param.conv2d.stride[1] = 1;
    node[29]->nn_param.conv2d.pad[0] = 1;
    node[29]->nn_param.conv2d.pad[1] = 1;
    node[29]->nn_param.conv2d.pad[2] = 1;
    node[29]->nn_param.conv2d.pad[3] = 1;
    node[29]->nn_param.conv2d.group = 1;
    node[29]->nn_param.conv2d.dilation[0] = 1;
    node[29]->nn_param.conv2d.dilation[1] = 1;
    node[29]->vx_param.has_relu = TRUE;
    node[29]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[29]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[29]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/5x5_48_inception_4a/relu_5x5_49
      var       - node[30]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 16, 1]]
      out_shape - [[14, 14, 48, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[30], VSI_NN_OP_CONV_RELU, 3, 1, 49);
    node[30]->nn_param.conv2d.ksize[0] = 5;
    node[30]->nn_param.conv2d.ksize[1] = 5;
    node[30]->nn_param.conv2d.weights = 48;
    node[30]->nn_param.conv2d.stride[0] = 1;
    node[30]->nn_param.conv2d.stride[1] = 1;
    node[30]->nn_param.conv2d.pad[0] = 2;
    node[30]->nn_param.conv2d.pad[1] = 2;
    node[30]->nn_param.conv2d.pad[2] = 2;
    node[30]->nn_param.conv2d.pad[3] = 2;
    node[30]->nn_param.conv2d.group = 1;
    node[30]->nn_param.conv2d.dilation[0] = 1;
    node[30]->nn_param.conv2d.dilation[1] = 1;
    node[30]->vx_param.has_relu = TRUE;
    node[30]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[30]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[30]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4a/output_53
      var       - node[31]
      name      - inception_4a/output
      operation - concat
      in_shape  - [[14, 14, 192, 1]]
                  [[14, 14, 208, 1]]
                  [[14, 14, 48, 1]]
                  [[14, 14, 64, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[31], VSI_NN_OP_CONCAT, 4, 1, 53);
    node[31]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_4b/pool_64
      var       - node[32]
      name      - inception_4b/pool
      operation - pooling
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[32], VSI_NN_OP_POOL, 1, 1, 64);
    node[32]->nn_param.pool.ksize[0] = 3;
    node[32]->nn_param.pool.ksize[1] = 3;
    node[32]->nn_param.pool.stride[0] = 1;
    node[32]->nn_param.pool.stride[1] = 1;
    node[32]->nn_param.pool.pad[0] = 1;
    node[32]->nn_param.pool.pad[1] = 1;
    node[32]->nn_param.pool.pad[2] = 1;
    node[32]->nn_param.pool.pad[3] = 1;
    node[32]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[32]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[32]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4b/1x1_54_inception_4b/relu_1x1_55
      var       - node[33]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 160, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[33], VSI_NN_OP_CONV_RELU, 3, 1, 55);
    node[33]->nn_param.conv2d.ksize[0] = 1;
    node[33]->nn_param.conv2d.ksize[1] = 1;
    node[33]->nn_param.conv2d.weights = 160;
    node[33]->nn_param.conv2d.stride[0] = 1;
    node[33]->nn_param.conv2d.stride[1] = 1;
    node[33]->nn_param.conv2d.pad[0] = 0;
    node[33]->nn_param.conv2d.pad[1] = 0;
    node[33]->nn_param.conv2d.pad[2] = 0;
    node[33]->nn_param.conv2d.pad[3] = 0;
    node[33]->nn_param.conv2d.group = 1;
    node[33]->nn_param.conv2d.dilation[0] = 1;
    node[33]->nn_param.conv2d.dilation[1] = 1;
    node[33]->vx_param.has_relu = TRUE;
    node[33]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[33]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[33]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/3x3_reduce_56_inception_4b/relu_3x3_reduce_57
      var       - node[34]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 112, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[34], VSI_NN_OP_CONV_RELU, 3, 1, 57);
    node[34]->nn_param.conv2d.ksize[0] = 1;
    node[34]->nn_param.conv2d.ksize[1] = 1;
    node[34]->nn_param.conv2d.weights = 112;
    node[34]->nn_param.conv2d.stride[0] = 1;
    node[34]->nn_param.conv2d.stride[1] = 1;
    node[34]->nn_param.conv2d.pad[0] = 0;
    node[34]->nn_param.conv2d.pad[1] = 0;
    node[34]->nn_param.conv2d.pad[2] = 0;
    node[34]->nn_param.conv2d.pad[3] = 0;
    node[34]->nn_param.conv2d.group = 1;
    node[34]->nn_param.conv2d.dilation[0] = 1;
    node[34]->nn_param.conv2d.dilation[1] = 1;
    node[34]->vx_param.has_relu = TRUE;
    node[34]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[34]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[34]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/5x5_reduce_60_inception_4b/relu_5x5_reduce_61
      var       - node[35]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 24, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[35], VSI_NN_OP_CONV_RELU, 3, 1, 61);
    node[35]->nn_param.conv2d.ksize[0] = 1;
    node[35]->nn_param.conv2d.ksize[1] = 1;
    node[35]->nn_param.conv2d.weights = 24;
    node[35]->nn_param.conv2d.stride[0] = 1;
    node[35]->nn_param.conv2d.stride[1] = 1;
    node[35]->nn_param.conv2d.pad[0] = 0;
    node[35]->nn_param.conv2d.pad[1] = 0;
    node[35]->nn_param.conv2d.pad[2] = 0;
    node[35]->nn_param.conv2d.pad[3] = 0;
    node[35]->nn_param.conv2d.group = 1;
    node[35]->nn_param.conv2d.dilation[0] = 1;
    node[35]->nn_param.conv2d.dilation[1] = 1;
    node[35]->vx_param.has_relu = TRUE;
    node[35]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[35]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[35]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/pool_proj_65_inception_4b/relu_pool_proj_66
      var       - node[36]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[36], VSI_NN_OP_CONV_RELU, 3, 1, 66);
    node[36]->nn_param.conv2d.ksize[0] = 1;
    node[36]->nn_param.conv2d.ksize[1] = 1;
    node[36]->nn_param.conv2d.weights = 64;
    node[36]->nn_param.conv2d.stride[0] = 1;
    node[36]->nn_param.conv2d.stride[1] = 1;
    node[36]->nn_param.conv2d.pad[0] = 0;
    node[36]->nn_param.conv2d.pad[1] = 0;
    node[36]->nn_param.conv2d.pad[2] = 0;
    node[36]->nn_param.conv2d.pad[3] = 0;
    node[36]->nn_param.conv2d.group = 1;
    node[36]->nn_param.conv2d.dilation[0] = 1;
    node[36]->nn_param.conv2d.dilation[1] = 1;
    node[36]->vx_param.has_relu = TRUE;
    node[36]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[36]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[36]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/3x3_58_inception_4b/relu_3x3_59
      var       - node[37]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 112, 1]]
      out_shape - [[14, 14, 224, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[37], VSI_NN_OP_CONV_RELU, 3, 1, 59);
    node[37]->nn_param.conv2d.ksize[0] = 3;
    node[37]->nn_param.conv2d.ksize[1] = 3;
    node[37]->nn_param.conv2d.weights = 224;
    node[37]->nn_param.conv2d.stride[0] = 1;
    node[37]->nn_param.conv2d.stride[1] = 1;
    node[37]->nn_param.conv2d.pad[0] = 1;
    node[37]->nn_param.conv2d.pad[1] = 1;
    node[37]->nn_param.conv2d.pad[2] = 1;
    node[37]->nn_param.conv2d.pad[3] = 1;
    node[37]->nn_param.conv2d.group = 1;
    node[37]->nn_param.conv2d.dilation[0] = 1;
    node[37]->nn_param.conv2d.dilation[1] = 1;
    node[37]->vx_param.has_relu = TRUE;
    node[37]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[37]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[37]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/5x5_62_inception_4b/relu_5x5_63
      var       - node[38]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 24, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[38], VSI_NN_OP_CONV_RELU, 3, 1, 63);
    node[38]->nn_param.conv2d.ksize[0] = 5;
    node[38]->nn_param.conv2d.ksize[1] = 5;
    node[38]->nn_param.conv2d.weights = 64;
    node[38]->nn_param.conv2d.stride[0] = 1;
    node[38]->nn_param.conv2d.stride[1] = 1;
    node[38]->nn_param.conv2d.pad[0] = 2;
    node[38]->nn_param.conv2d.pad[1] = 2;
    node[38]->nn_param.conv2d.pad[2] = 2;
    node[38]->nn_param.conv2d.pad[3] = 2;
    node[38]->nn_param.conv2d.group = 1;
    node[38]->nn_param.conv2d.dilation[0] = 1;
    node[38]->nn_param.conv2d.dilation[1] = 1;
    node[38]->vx_param.has_relu = TRUE;
    node[38]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[38]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[38]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4b/output_67
      var       - node[39]
      name      - inception_4b/output
      operation - concat
      in_shape  - [[14, 14, 160, 1]]
                  [[14, 14, 224, 1]]
                  [[14, 14, 64, 1]]
                  [[14, 14, 64, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[39], VSI_NN_OP_CONCAT, 4, 1, 67);
    node[39]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_4c/pool_78
      var       - node[40]
      name      - inception_4c/pool
      operation - pooling
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[40], VSI_NN_OP_POOL, 1, 1, 78);
    node[40]->nn_param.pool.ksize[0] = 3;
    node[40]->nn_param.pool.ksize[1] = 3;
    node[40]->nn_param.pool.stride[0] = 1;
    node[40]->nn_param.pool.stride[1] = 1;
    node[40]->nn_param.pool.pad[0] = 1;
    node[40]->nn_param.pool.pad[1] = 1;
    node[40]->nn_param.pool.pad[2] = 1;
    node[40]->nn_param.pool.pad[3] = 1;
    node[40]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[40]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[40]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4c/1x1_68_inception_4c/relu_1x1_69
      var       - node[41]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[41], VSI_NN_OP_CONV_RELU, 3, 1, 69);
    node[41]->nn_param.conv2d.ksize[0] = 1;
    node[41]->nn_param.conv2d.ksize[1] = 1;
    node[41]->nn_param.conv2d.weights = 128;
    node[41]->nn_param.conv2d.stride[0] = 1;
    node[41]->nn_param.conv2d.stride[1] = 1;
    node[41]->nn_param.conv2d.pad[0] = 0;
    node[41]->nn_param.conv2d.pad[1] = 0;
    node[41]->nn_param.conv2d.pad[2] = 0;
    node[41]->nn_param.conv2d.pad[3] = 0;
    node[41]->nn_param.conv2d.group = 1;
    node[41]->nn_param.conv2d.dilation[0] = 1;
    node[41]->nn_param.conv2d.dilation[1] = 1;
    node[41]->vx_param.has_relu = TRUE;
    node[41]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[41]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[41]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/3x3_reduce_70_inception_4c/relu_3x3_reduce_71
      var       - node[42]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[42], VSI_NN_OP_CONV_RELU, 3, 1, 71);
    node[42]->nn_param.conv2d.ksize[0] = 1;
    node[42]->nn_param.conv2d.ksize[1] = 1;
    node[42]->nn_param.conv2d.weights = 128;
    node[42]->nn_param.conv2d.stride[0] = 1;
    node[42]->nn_param.conv2d.stride[1] = 1;
    node[42]->nn_param.conv2d.pad[0] = 0;
    node[42]->nn_param.conv2d.pad[1] = 0;
    node[42]->nn_param.conv2d.pad[2] = 0;
    node[42]->nn_param.conv2d.pad[3] = 0;
    node[42]->nn_param.conv2d.group = 1;
    node[42]->nn_param.conv2d.dilation[0] = 1;
    node[42]->nn_param.conv2d.dilation[1] = 1;
    node[42]->vx_param.has_relu = TRUE;
    node[42]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[42]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[42]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/5x5_reduce_74_inception_4c/relu_5x5_reduce_75
      var       - node[43]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 24, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[43], VSI_NN_OP_CONV_RELU, 3, 1, 75);
    node[43]->nn_param.conv2d.ksize[0] = 1;
    node[43]->nn_param.conv2d.ksize[1] = 1;
    node[43]->nn_param.conv2d.weights = 24;
    node[43]->nn_param.conv2d.stride[0] = 1;
    node[43]->nn_param.conv2d.stride[1] = 1;
    node[43]->nn_param.conv2d.pad[0] = 0;
    node[43]->nn_param.conv2d.pad[1] = 0;
    node[43]->nn_param.conv2d.pad[2] = 0;
    node[43]->nn_param.conv2d.pad[3] = 0;
    node[43]->nn_param.conv2d.group = 1;
    node[43]->nn_param.conv2d.dilation[0] = 1;
    node[43]->nn_param.conv2d.dilation[1] = 1;
    node[43]->vx_param.has_relu = TRUE;
    node[43]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[43]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[43]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/pool_proj_79_inception_4c/relu_pool_proj_80
      var       - node[44]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[44], VSI_NN_OP_CONV_RELU, 3, 1, 80);
    node[44]->nn_param.conv2d.ksize[0] = 1;
    node[44]->nn_param.conv2d.ksize[1] = 1;
    node[44]->nn_param.conv2d.weights = 64;
    node[44]->nn_param.conv2d.stride[0] = 1;
    node[44]->nn_param.conv2d.stride[1] = 1;
    node[44]->nn_param.conv2d.pad[0] = 0;
    node[44]->nn_param.conv2d.pad[1] = 0;
    node[44]->nn_param.conv2d.pad[2] = 0;
    node[44]->nn_param.conv2d.pad[3] = 0;
    node[44]->nn_param.conv2d.group = 1;
    node[44]->nn_param.conv2d.dilation[0] = 1;
    node[44]->nn_param.conv2d.dilation[1] = 1;
    node[44]->vx_param.has_relu = TRUE;
    node[44]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[44]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[44]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/3x3_72_inception_4c/relu_3x3_73
      var       - node[45]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 128, 1]]
      out_shape - [[14, 14, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[45], VSI_NN_OP_CONV_RELU, 3, 1, 73);
    node[45]->nn_param.conv2d.ksize[0] = 3;
    node[45]->nn_param.conv2d.ksize[1] = 3;
    node[45]->nn_param.conv2d.weights = 256;
    node[45]->nn_param.conv2d.stride[0] = 1;
    node[45]->nn_param.conv2d.stride[1] = 1;
    node[45]->nn_param.conv2d.pad[0] = 1;
    node[45]->nn_param.conv2d.pad[1] = 1;
    node[45]->nn_param.conv2d.pad[2] = 1;
    node[45]->nn_param.conv2d.pad[3] = 1;
    node[45]->nn_param.conv2d.group = 1;
    node[45]->nn_param.conv2d.dilation[0] = 1;
    node[45]->nn_param.conv2d.dilation[1] = 1;
    node[45]->vx_param.has_relu = TRUE;
    node[45]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[45]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[45]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/5x5_76_inception_4c/relu_5x5_77
      var       - node[46]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 24, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[46], VSI_NN_OP_CONV_RELU, 3, 1, 77);
    node[46]->nn_param.conv2d.ksize[0] = 5;
    node[46]->nn_param.conv2d.ksize[1] = 5;
    node[46]->nn_param.conv2d.weights = 64;
    node[46]->nn_param.conv2d.stride[0] = 1;
    node[46]->nn_param.conv2d.stride[1] = 1;
    node[46]->nn_param.conv2d.pad[0] = 2;
    node[46]->nn_param.conv2d.pad[1] = 2;
    node[46]->nn_param.conv2d.pad[2] = 2;
    node[46]->nn_param.conv2d.pad[3] = 2;
    node[46]->nn_param.conv2d.group = 1;
    node[46]->nn_param.conv2d.dilation[0] = 1;
    node[46]->nn_param.conv2d.dilation[1] = 1;
    node[46]->vx_param.has_relu = TRUE;
    node[46]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[46]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[46]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4c/output_81
      var       - node[47]
      name      - inception_4c/output
      operation - concat
      in_shape  - [[14, 14, 128, 1]]
                  [[14, 14, 256, 1]]
                  [[14, 14, 64, 1]]
                  [[14, 14, 64, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[47], VSI_NN_OP_CONCAT, 4, 1, 81);
    node[47]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_4d/pool_92
      var       - node[48]
      name      - inception_4d/pool
      operation - pooling
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 512, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[48], VSI_NN_OP_POOL, 1, 1, 92);
    node[48]->nn_param.pool.ksize[0] = 3;
    node[48]->nn_param.pool.ksize[1] = 3;
    node[48]->nn_param.pool.stride[0] = 1;
    node[48]->nn_param.pool.stride[1] = 1;
    node[48]->nn_param.pool.pad[0] = 1;
    node[48]->nn_param.pool.pad[1] = 1;
    node[48]->nn_param.pool.pad[2] = 1;
    node[48]->nn_param.pool.pad[3] = 1;
    node[48]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[48]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[48]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4d/1x1_82_inception_4d/relu_1x1_83
      var       - node[49]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 112, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[49], VSI_NN_OP_CONV_RELU, 3, 1, 83);
    node[49]->nn_param.conv2d.ksize[0] = 1;
    node[49]->nn_param.conv2d.ksize[1] = 1;
    node[49]->nn_param.conv2d.weights = 112;
    node[49]->nn_param.conv2d.stride[0] = 1;
    node[49]->nn_param.conv2d.stride[1] = 1;
    node[49]->nn_param.conv2d.pad[0] = 0;
    node[49]->nn_param.conv2d.pad[1] = 0;
    node[49]->nn_param.conv2d.pad[2] = 0;
    node[49]->nn_param.conv2d.pad[3] = 0;
    node[49]->nn_param.conv2d.group = 1;
    node[49]->nn_param.conv2d.dilation[0] = 1;
    node[49]->nn_param.conv2d.dilation[1] = 1;
    node[49]->vx_param.has_relu = TRUE;
    node[49]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[49]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[49]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/3x3_reduce_84_inception_4d/relu_3x3_reduce_85
      var       - node[50]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 144, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[50], VSI_NN_OP_CONV_RELU, 3, 1, 85);
    node[50]->nn_param.conv2d.ksize[0] = 1;
    node[50]->nn_param.conv2d.ksize[1] = 1;
    node[50]->nn_param.conv2d.weights = 144;
    node[50]->nn_param.conv2d.stride[0] = 1;
    node[50]->nn_param.conv2d.stride[1] = 1;
    node[50]->nn_param.conv2d.pad[0] = 0;
    node[50]->nn_param.conv2d.pad[1] = 0;
    node[50]->nn_param.conv2d.pad[2] = 0;
    node[50]->nn_param.conv2d.pad[3] = 0;
    node[50]->nn_param.conv2d.group = 1;
    node[50]->nn_param.conv2d.dilation[0] = 1;
    node[50]->nn_param.conv2d.dilation[1] = 1;
    node[50]->vx_param.has_relu = TRUE;
    node[50]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[50]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[50]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/5x5_reduce_88_inception_4d/relu_5x5_reduce_89
      var       - node[51]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[51], VSI_NN_OP_CONV_RELU, 3, 1, 89);
    node[51]->nn_param.conv2d.ksize[0] = 1;
    node[51]->nn_param.conv2d.ksize[1] = 1;
    node[51]->nn_param.conv2d.weights = 32;
    node[51]->nn_param.conv2d.stride[0] = 1;
    node[51]->nn_param.conv2d.stride[1] = 1;
    node[51]->nn_param.conv2d.pad[0] = 0;
    node[51]->nn_param.conv2d.pad[1] = 0;
    node[51]->nn_param.conv2d.pad[2] = 0;
    node[51]->nn_param.conv2d.pad[3] = 0;
    node[51]->nn_param.conv2d.group = 1;
    node[51]->nn_param.conv2d.dilation[0] = 1;
    node[51]->nn_param.conv2d.dilation[1] = 1;
    node[51]->vx_param.has_relu = TRUE;
    node[51]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[51]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[51]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/pool_proj_93_inception_4d/relu_pool_proj_94
      var       - node[52]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 512, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[52], VSI_NN_OP_CONV_RELU, 3, 1, 94);
    node[52]->nn_param.conv2d.ksize[0] = 1;
    node[52]->nn_param.conv2d.ksize[1] = 1;
    node[52]->nn_param.conv2d.weights = 64;
    node[52]->nn_param.conv2d.stride[0] = 1;
    node[52]->nn_param.conv2d.stride[1] = 1;
    node[52]->nn_param.conv2d.pad[0] = 0;
    node[52]->nn_param.conv2d.pad[1] = 0;
    node[52]->nn_param.conv2d.pad[2] = 0;
    node[52]->nn_param.conv2d.pad[3] = 0;
    node[52]->nn_param.conv2d.group = 1;
    node[52]->nn_param.conv2d.dilation[0] = 1;
    node[52]->nn_param.conv2d.dilation[1] = 1;
    node[52]->vx_param.has_relu = TRUE;
    node[52]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[52]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[52]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/3x3_86_inception_4d/relu_3x3_87
      var       - node[53]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 144, 1]]
      out_shape - [[14, 14, 288, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[53], VSI_NN_OP_CONV_RELU, 3, 1, 87);
    node[53]->nn_param.conv2d.ksize[0] = 3;
    node[53]->nn_param.conv2d.ksize[1] = 3;
    node[53]->nn_param.conv2d.weights = 288;
    node[53]->nn_param.conv2d.stride[0] = 1;
    node[53]->nn_param.conv2d.stride[1] = 1;
    node[53]->nn_param.conv2d.pad[0] = 1;
    node[53]->nn_param.conv2d.pad[1] = 1;
    node[53]->nn_param.conv2d.pad[2] = 1;
    node[53]->nn_param.conv2d.pad[3] = 1;
    node[53]->nn_param.conv2d.group = 1;
    node[53]->nn_param.conv2d.dilation[0] = 1;
    node[53]->nn_param.conv2d.dilation[1] = 1;
    node[53]->vx_param.has_relu = TRUE;
    node[53]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[53]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[53]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/5x5_90_inception_4d/relu_5x5_91
      var       - node[54]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 32, 1]]
      out_shape - [[14, 14, 64, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[54], VSI_NN_OP_CONV_RELU, 3, 1, 91);
    node[54]->nn_param.conv2d.ksize[0] = 5;
    node[54]->nn_param.conv2d.ksize[1] = 5;
    node[54]->nn_param.conv2d.weights = 64;
    node[54]->nn_param.conv2d.stride[0] = 1;
    node[54]->nn_param.conv2d.stride[1] = 1;
    node[54]->nn_param.conv2d.pad[0] = 2;
    node[54]->nn_param.conv2d.pad[1] = 2;
    node[54]->nn_param.conv2d.pad[2] = 2;
    node[54]->nn_param.conv2d.pad[3] = 2;
    node[54]->nn_param.conv2d.group = 1;
    node[54]->nn_param.conv2d.dilation[0] = 1;
    node[54]->nn_param.conv2d.dilation[1] = 1;
    node[54]->vx_param.has_relu = TRUE;
    node[54]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[54]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[54]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4d/output_95
      var       - node[55]
      name      - inception_4d/output
      operation - concat
      in_shape  - [[14, 14, 112, 1]]
                  [[14, 14, 288, 1]]
                  [[14, 14, 64, 1]]
                  [[14, 14, 64, 1]]
      out_shape - [[14, 14, 528, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[55], VSI_NN_OP_CONCAT, 4, 1, 95);
    node[55]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_4e/pool_106
      var       - node[56]
      name      - inception_4e/pool
      operation - pooling
      in_shape  - [[14, 14, 528, 1]]
      out_shape - [[14, 14, 528, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[56], VSI_NN_OP_POOL, 1, 1, 106);
    node[56]->nn_param.pool.ksize[0] = 3;
    node[56]->nn_param.pool.ksize[1] = 3;
    node[56]->nn_param.pool.stride[0] = 1;
    node[56]->nn_param.pool.stride[1] = 1;
    node[56]->nn_param.pool.pad[0] = 1;
    node[56]->nn_param.pool.pad[1] = 1;
    node[56]->nn_param.pool.pad[2] = 1;
    node[56]->nn_param.pool.pad[3] = 1;
    node[56]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[56]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[56]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_4e/1x1_96_inception_4e/relu_1x1_97
      var       - node[57]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 528, 1]]
      out_shape - [[14, 14, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[57], VSI_NN_OP_CONV_RELU, 3, 1, 97);
    node[57]->nn_param.conv2d.ksize[0] = 1;
    node[57]->nn_param.conv2d.ksize[1] = 1;
    node[57]->nn_param.conv2d.weights = 256;
    node[57]->nn_param.conv2d.stride[0] = 1;
    node[57]->nn_param.conv2d.stride[1] = 1;
    node[57]->nn_param.conv2d.pad[0] = 0;
    node[57]->nn_param.conv2d.pad[1] = 0;
    node[57]->nn_param.conv2d.pad[2] = 0;
    node[57]->nn_param.conv2d.pad[3] = 0;
    node[57]->nn_param.conv2d.group = 1;
    node[57]->nn_param.conv2d.dilation[0] = 1;
    node[57]->nn_param.conv2d.dilation[1] = 1;
    node[57]->vx_param.has_relu = TRUE;
    node[57]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[57]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[57]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/3x3_reduce_98_inception_4e/relu_3x3_reduce_99
      var       - node[58]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 528, 1]]
      out_shape - [[14, 14, 160, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[58], VSI_NN_OP_CONV_RELU, 3, 1, 99);
    node[58]->nn_param.conv2d.ksize[0] = 1;
    node[58]->nn_param.conv2d.ksize[1] = 1;
    node[58]->nn_param.conv2d.weights = 160;
    node[58]->nn_param.conv2d.stride[0] = 1;
    node[58]->nn_param.conv2d.stride[1] = 1;
    node[58]->nn_param.conv2d.pad[0] = 0;
    node[58]->nn_param.conv2d.pad[1] = 0;
    node[58]->nn_param.conv2d.pad[2] = 0;
    node[58]->nn_param.conv2d.pad[3] = 0;
    node[58]->nn_param.conv2d.group = 1;
    node[58]->nn_param.conv2d.dilation[0] = 1;
    node[58]->nn_param.conv2d.dilation[1] = 1;
    node[58]->vx_param.has_relu = TRUE;
    node[58]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[58]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[58]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/5x5_reduce_102_inception_4e/relu_5x5_reduce_103
      var       - node[59]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 528, 1]]
      out_shape - [[14, 14, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[59], VSI_NN_OP_CONV_RELU, 3, 1, 103);
    node[59]->nn_param.conv2d.ksize[0] = 1;
    node[59]->nn_param.conv2d.ksize[1] = 1;
    node[59]->nn_param.conv2d.weights = 32;
    node[59]->nn_param.conv2d.stride[0] = 1;
    node[59]->nn_param.conv2d.stride[1] = 1;
    node[59]->nn_param.conv2d.pad[0] = 0;
    node[59]->nn_param.conv2d.pad[1] = 0;
    node[59]->nn_param.conv2d.pad[2] = 0;
    node[59]->nn_param.conv2d.pad[3] = 0;
    node[59]->nn_param.conv2d.group = 1;
    node[59]->nn_param.conv2d.dilation[0] = 1;
    node[59]->nn_param.conv2d.dilation[1] = 1;
    node[59]->vx_param.has_relu = TRUE;
    node[59]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[59]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[59]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/pool_proj_107_inception_4e/relu_pool_proj_108
      var       - node[60]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 528, 1]]
      out_shape - [[14, 14, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[60], VSI_NN_OP_CONV_RELU, 3, 1, 108);
    node[60]->nn_param.conv2d.ksize[0] = 1;
    node[60]->nn_param.conv2d.ksize[1] = 1;
    node[60]->nn_param.conv2d.weights = 128;
    node[60]->nn_param.conv2d.stride[0] = 1;
    node[60]->nn_param.conv2d.stride[1] = 1;
    node[60]->nn_param.conv2d.pad[0] = 0;
    node[60]->nn_param.conv2d.pad[1] = 0;
    node[60]->nn_param.conv2d.pad[2] = 0;
    node[60]->nn_param.conv2d.pad[3] = 0;
    node[60]->nn_param.conv2d.group = 1;
    node[60]->nn_param.conv2d.dilation[0] = 1;
    node[60]->nn_param.conv2d.dilation[1] = 1;
    node[60]->vx_param.has_relu = TRUE;
    node[60]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[60]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[60]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/3x3_100_inception_4e/relu_3x3_101
      var       - node[61]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 160, 1]]
      out_shape - [[14, 14, 320, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[61], VSI_NN_OP_CONV_RELU, 3, 1, 101);
    node[61]->nn_param.conv2d.ksize[0] = 3;
    node[61]->nn_param.conv2d.ksize[1] = 3;
    node[61]->nn_param.conv2d.weights = 320;
    node[61]->nn_param.conv2d.stride[0] = 1;
    node[61]->nn_param.conv2d.stride[1] = 1;
    node[61]->nn_param.conv2d.pad[0] = 1;
    node[61]->nn_param.conv2d.pad[1] = 1;
    node[61]->nn_param.conv2d.pad[2] = 1;
    node[61]->nn_param.conv2d.pad[3] = 1;
    node[61]->nn_param.conv2d.group = 1;
    node[61]->nn_param.conv2d.dilation[0] = 1;
    node[61]->nn_param.conv2d.dilation[1] = 1;
    node[61]->vx_param.has_relu = TRUE;
    node[61]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[61]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[61]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/5x5_104_inception_4e/relu_5x5_105
      var       - node[62]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[14, 14, 32, 1]]
      out_shape - [[14, 14, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[62], VSI_NN_OP_CONV_RELU, 3, 1, 105);
    node[62]->nn_param.conv2d.ksize[0] = 5;
    node[62]->nn_param.conv2d.ksize[1] = 5;
    node[62]->nn_param.conv2d.weights = 128;
    node[62]->nn_param.conv2d.stride[0] = 1;
    node[62]->nn_param.conv2d.stride[1] = 1;
    node[62]->nn_param.conv2d.pad[0] = 2;
    node[62]->nn_param.conv2d.pad[1] = 2;
    node[62]->nn_param.conv2d.pad[2] = 2;
    node[62]->nn_param.conv2d.pad[3] = 2;
    node[62]->nn_param.conv2d.group = 1;
    node[62]->nn_param.conv2d.dilation[0] = 1;
    node[62]->nn_param.conv2d.dilation[1] = 1;
    node[62]->vx_param.has_relu = TRUE;
    node[62]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[62]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[62]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_4e/output_109
      var       - node[63]
      name      - inception_4e/output
      operation - concat
      in_shape  - [[14, 14, 256, 1]]
                  [[14, 14, 320, 1]]
                  [[14, 14, 128, 1]]
                  [[14, 14, 128, 1]]
      out_shape - [[14, 14, 832, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[63], VSI_NN_OP_CONCAT, 4, 1, 109);
    node[63]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - pool4/3x3_s2_110
      var       - node[64]
      name      - pool4/3x3_s2
      operation - pooling
      in_shape  - [[14, 14, 832, 1]]
      out_shape - [[7, 7, 832, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[64], VSI_NN_OP_POOL, 1, 1, 110);
    node[64]->nn_param.pool.ksize[0] = 3;
    node[64]->nn_param.pool.ksize[1] = 3;
    node[64]->nn_param.pool.stride[0] = 2;
    node[64]->nn_param.pool.stride[1] = 2;
    node[64]->nn_param.pool.pad[0] = 0;
    node[64]->nn_param.pool.pad[1] = 1;
    node[64]->nn_param.pool.pad[2] = 0;
    node[64]->nn_param.pool.pad[3] = 1;
    node[64]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[64]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[64]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_5a/pool_121
      var       - node[65]
      name      - inception_5a/pool
      operation - pooling
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 832, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[65], VSI_NN_OP_POOL, 1, 1, 121);
    node[65]->nn_param.pool.ksize[0] = 3;
    node[65]->nn_param.pool.ksize[1] = 3;
    node[65]->nn_param.pool.stride[0] = 1;
    node[65]->nn_param.pool.stride[1] = 1;
    node[65]->nn_param.pool.pad[0] = 1;
    node[65]->nn_param.pool.pad[1] = 1;
    node[65]->nn_param.pool.pad[2] = 1;
    node[65]->nn_param.pool.pad[3] = 1;
    node[65]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[65]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[65]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_5a/1x1_111_inception_5a/relu_1x1_112
      var       - node[66]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 256, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[66], VSI_NN_OP_CONV_RELU, 3, 1, 112);
    node[66]->nn_param.conv2d.ksize[0] = 1;
    node[66]->nn_param.conv2d.ksize[1] = 1;
    node[66]->nn_param.conv2d.weights = 256;
    node[66]->nn_param.conv2d.stride[0] = 1;
    node[66]->nn_param.conv2d.stride[1] = 1;
    node[66]->nn_param.conv2d.pad[0] = 0;
    node[66]->nn_param.conv2d.pad[1] = 0;
    node[66]->nn_param.conv2d.pad[2] = 0;
    node[66]->nn_param.conv2d.pad[3] = 0;
    node[66]->nn_param.conv2d.group = 1;
    node[66]->nn_param.conv2d.dilation[0] = 1;
    node[66]->nn_param.conv2d.dilation[1] = 1;
    node[66]->vx_param.has_relu = TRUE;
    node[66]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[66]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[66]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/3x3_reduce_113_inception_5a/relu_3x3_reduce_114
      var       - node[67]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 160, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[67], VSI_NN_OP_CONV_RELU, 3, 1, 114);
    node[67]->nn_param.conv2d.ksize[0] = 1;
    node[67]->nn_param.conv2d.ksize[1] = 1;
    node[67]->nn_param.conv2d.weights = 160;
    node[67]->nn_param.conv2d.stride[0] = 1;
    node[67]->nn_param.conv2d.stride[1] = 1;
    node[67]->nn_param.conv2d.pad[0] = 0;
    node[67]->nn_param.conv2d.pad[1] = 0;
    node[67]->nn_param.conv2d.pad[2] = 0;
    node[67]->nn_param.conv2d.pad[3] = 0;
    node[67]->nn_param.conv2d.group = 1;
    node[67]->nn_param.conv2d.dilation[0] = 1;
    node[67]->nn_param.conv2d.dilation[1] = 1;
    node[67]->vx_param.has_relu = TRUE;
    node[67]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[67]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[67]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/5x5_reduce_117_inception_5a/relu_5x5_reduce_118
      var       - node[68]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 32, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[68], VSI_NN_OP_CONV_RELU, 3, 1, 118);
    node[68]->nn_param.conv2d.ksize[0] = 1;
    node[68]->nn_param.conv2d.ksize[1] = 1;
    node[68]->nn_param.conv2d.weights = 32;
    node[68]->nn_param.conv2d.stride[0] = 1;
    node[68]->nn_param.conv2d.stride[1] = 1;
    node[68]->nn_param.conv2d.pad[0] = 0;
    node[68]->nn_param.conv2d.pad[1] = 0;
    node[68]->nn_param.conv2d.pad[2] = 0;
    node[68]->nn_param.conv2d.pad[3] = 0;
    node[68]->nn_param.conv2d.group = 1;
    node[68]->nn_param.conv2d.dilation[0] = 1;
    node[68]->nn_param.conv2d.dilation[1] = 1;
    node[68]->vx_param.has_relu = TRUE;
    node[68]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[68]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[68]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/pool_proj_122_inception_5a/relu_pool_proj_123
      var       - node[69]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[69], VSI_NN_OP_CONV_RELU, 3, 1, 123);
    node[69]->nn_param.conv2d.ksize[0] = 1;
    node[69]->nn_param.conv2d.ksize[1] = 1;
    node[69]->nn_param.conv2d.weights = 128;
    node[69]->nn_param.conv2d.stride[0] = 1;
    node[69]->nn_param.conv2d.stride[1] = 1;
    node[69]->nn_param.conv2d.pad[0] = 0;
    node[69]->nn_param.conv2d.pad[1] = 0;
    node[69]->nn_param.conv2d.pad[2] = 0;
    node[69]->nn_param.conv2d.pad[3] = 0;
    node[69]->nn_param.conv2d.group = 1;
    node[69]->nn_param.conv2d.dilation[0] = 1;
    node[69]->nn_param.conv2d.dilation[1] = 1;
    node[69]->vx_param.has_relu = TRUE;
    node[69]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[69]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[69]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/3x3_115_inception_5a/relu_3x3_116
      var       - node[70]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 160, 1]]
      out_shape - [[7, 7, 320, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[70], VSI_NN_OP_CONV_RELU, 3, 1, 116);
    node[70]->nn_param.conv2d.ksize[0] = 3;
    node[70]->nn_param.conv2d.ksize[1] = 3;
    node[70]->nn_param.conv2d.weights = 320;
    node[70]->nn_param.conv2d.stride[0] = 1;
    node[70]->nn_param.conv2d.stride[1] = 1;
    node[70]->nn_param.conv2d.pad[0] = 1;
    node[70]->nn_param.conv2d.pad[1] = 1;
    node[70]->nn_param.conv2d.pad[2] = 1;
    node[70]->nn_param.conv2d.pad[3] = 1;
    node[70]->nn_param.conv2d.group = 1;
    node[70]->nn_param.conv2d.dilation[0] = 1;
    node[70]->nn_param.conv2d.dilation[1] = 1;
    node[70]->vx_param.has_relu = TRUE;
    node[70]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[70]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[70]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/5x5_119_inception_5a/relu_5x5_120
      var       - node[71]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 32, 1]]
      out_shape - [[7, 7, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[71], VSI_NN_OP_CONV_RELU, 3, 1, 120);
    node[71]->nn_param.conv2d.ksize[0] = 5;
    node[71]->nn_param.conv2d.ksize[1] = 5;
    node[71]->nn_param.conv2d.weights = 128;
    node[71]->nn_param.conv2d.stride[0] = 1;
    node[71]->nn_param.conv2d.stride[1] = 1;
    node[71]->nn_param.conv2d.pad[0] = 2;
    node[71]->nn_param.conv2d.pad[1] = 2;
    node[71]->nn_param.conv2d.pad[2] = 2;
    node[71]->nn_param.conv2d.pad[3] = 2;
    node[71]->nn_param.conv2d.group = 1;
    node[71]->nn_param.conv2d.dilation[0] = 1;
    node[71]->nn_param.conv2d.dilation[1] = 1;
    node[71]->vx_param.has_relu = TRUE;
    node[71]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[71]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[71]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5a/output_124
      var       - node[72]
      name      - inception_5a/output
      operation - concat
      in_shape  - [[7, 7, 256, 1]]
                  [[7, 7, 320, 1]]
                  [[7, 7, 128, 1]]
                  [[7, 7, 128, 1]]
      out_shape - [[7, 7, 832, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[72], VSI_NN_OP_CONCAT, 4, 1, 124);
    node[72]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - inception_5b/pool_135
      var       - node[73]
      name      - inception_5b/pool
      operation - pooling
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 832, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[73], VSI_NN_OP_POOL, 1, 1, 135);
    node[73]->nn_param.pool.ksize[0] = 3;
    node[73]->nn_param.pool.ksize[1] = 3;
    node[73]->nn_param.pool.stride[0] = 1;
    node[73]->nn_param.pool.stride[1] = 1;
    node[73]->nn_param.pool.pad[0] = 1;
    node[73]->nn_param.pool.pad[1] = 1;
    node[73]->nn_param.pool.pad[2] = 1;
    node[73]->nn_param.pool.pad[3] = 1;
    node[73]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_MAX;
    node[73]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[73]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - inception_5b/1x1_125_inception_5b/relu_1x1_126
      var       - node[74]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[74], VSI_NN_OP_CONV_RELU, 3, 1, 126);
    node[74]->nn_param.conv2d.ksize[0] = 1;
    node[74]->nn_param.conv2d.ksize[1] = 1;
    node[74]->nn_param.conv2d.weights = 384;
    node[74]->nn_param.conv2d.stride[0] = 1;
    node[74]->nn_param.conv2d.stride[1] = 1;
    node[74]->nn_param.conv2d.pad[0] = 0;
    node[74]->nn_param.conv2d.pad[1] = 0;
    node[74]->nn_param.conv2d.pad[2] = 0;
    node[74]->nn_param.conv2d.pad[3] = 0;
    node[74]->nn_param.conv2d.group = 1;
    node[74]->nn_param.conv2d.dilation[0] = 1;
    node[74]->nn_param.conv2d.dilation[1] = 1;
    node[74]->vx_param.has_relu = TRUE;
    node[74]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[74]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[74]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/3x3_reduce_127_inception_5b/relu_3x3_reduce_128
      var       - node[75]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 192, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[75], VSI_NN_OP_CONV_RELU, 3, 1, 128);
    node[75]->nn_param.conv2d.ksize[0] = 1;
    node[75]->nn_param.conv2d.ksize[1] = 1;
    node[75]->nn_param.conv2d.weights = 192;
    node[75]->nn_param.conv2d.stride[0] = 1;
    node[75]->nn_param.conv2d.stride[1] = 1;
    node[75]->nn_param.conv2d.pad[0] = 0;
    node[75]->nn_param.conv2d.pad[1] = 0;
    node[75]->nn_param.conv2d.pad[2] = 0;
    node[75]->nn_param.conv2d.pad[3] = 0;
    node[75]->nn_param.conv2d.group = 1;
    node[75]->nn_param.conv2d.dilation[0] = 1;
    node[75]->nn_param.conv2d.dilation[1] = 1;
    node[75]->vx_param.has_relu = TRUE;
    node[75]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[75]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[75]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/5x5_reduce_131_inception_5b/relu_5x5_reduce_132
      var       - node[76]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 48, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[76], VSI_NN_OP_CONV_RELU, 3, 1, 132);
    node[76]->nn_param.conv2d.ksize[0] = 1;
    node[76]->nn_param.conv2d.ksize[1] = 1;
    node[76]->nn_param.conv2d.weights = 48;
    node[76]->nn_param.conv2d.stride[0] = 1;
    node[76]->nn_param.conv2d.stride[1] = 1;
    node[76]->nn_param.conv2d.pad[0] = 0;
    node[76]->nn_param.conv2d.pad[1] = 0;
    node[76]->nn_param.conv2d.pad[2] = 0;
    node[76]->nn_param.conv2d.pad[3] = 0;
    node[76]->nn_param.conv2d.group = 1;
    node[76]->nn_param.conv2d.dilation[0] = 1;
    node[76]->nn_param.conv2d.dilation[1] = 1;
    node[76]->vx_param.has_relu = TRUE;
    node[76]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[76]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[76]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/pool_proj_136_inception_5b/relu_pool_proj_137
      var       - node[77]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 832, 1]]
      out_shape - [[7, 7, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[77], VSI_NN_OP_CONV_RELU, 3, 1, 137);
    node[77]->nn_param.conv2d.ksize[0] = 1;
    node[77]->nn_param.conv2d.ksize[1] = 1;
    node[77]->nn_param.conv2d.weights = 128;
    node[77]->nn_param.conv2d.stride[0] = 1;
    node[77]->nn_param.conv2d.stride[1] = 1;
    node[77]->nn_param.conv2d.pad[0] = 0;
    node[77]->nn_param.conv2d.pad[1] = 0;
    node[77]->nn_param.conv2d.pad[2] = 0;
    node[77]->nn_param.conv2d.pad[3] = 0;
    node[77]->nn_param.conv2d.group = 1;
    node[77]->nn_param.conv2d.dilation[0] = 1;
    node[77]->nn_param.conv2d.dilation[1] = 1;
    node[77]->vx_param.has_relu = TRUE;
    node[77]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[77]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[77]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/3x3_129_inception_5b/relu_3x3_130
      var       - node[78]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 192, 1]]
      out_shape - [[7, 7, 384, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[78], VSI_NN_OP_CONV_RELU, 3, 1, 130);
    node[78]->nn_param.conv2d.ksize[0] = 3;
    node[78]->nn_param.conv2d.ksize[1] = 3;
    node[78]->nn_param.conv2d.weights = 384;
    node[78]->nn_param.conv2d.stride[0] = 1;
    node[78]->nn_param.conv2d.stride[1] = 1;
    node[78]->nn_param.conv2d.pad[0] = 1;
    node[78]->nn_param.conv2d.pad[1] = 1;
    node[78]->nn_param.conv2d.pad[2] = 1;
    node[78]->nn_param.conv2d.pad[3] = 1;
    node[78]->nn_param.conv2d.group = 1;
    node[78]->nn_param.conv2d.dilation[0] = 1;
    node[78]->nn_param.conv2d.dilation[1] = 1;
    node[78]->vx_param.has_relu = TRUE;
    node[78]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[78]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[78]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/5x5_133_inception_5b/relu_5x5_134
      var       - node[79]
      name      - convolutionrelu
      operation - convolutionrelu
      in_shape  - [[7, 7, 48, 1]]
      out_shape - [[7, 7, 128, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[79], VSI_NN_OP_CONV_RELU, 3, 1, 134);
    node[79]->nn_param.conv2d.ksize[0] = 5;
    node[79]->nn_param.conv2d.ksize[1] = 5;
    node[79]->nn_param.conv2d.weights = 128;
    node[79]->nn_param.conv2d.stride[0] = 1;
    node[79]->nn_param.conv2d.stride[1] = 1;
    node[79]->nn_param.conv2d.pad[0] = 2;
    node[79]->nn_param.conv2d.pad[1] = 2;
    node[79]->nn_param.conv2d.pad[2] = 2;
    node[79]->nn_param.conv2d.pad[3] = 2;
    node[79]->nn_param.conv2d.group = 1;
    node[79]->nn_param.conv2d.dilation[0] = 1;
    node[79]->nn_param.conv2d.dilation[1] = 1;
    node[79]->vx_param.has_relu = TRUE;
    node[79]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[79]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[79]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - inception_5b/output_138
      var       - node[80]
      name      - inception_5b/output
      operation - concat
      in_shape  - [[7, 7, 384, 1]]
                  [[7, 7, 384, 1]]
                  [[7, 7, 128, 1]]
                  [[7, 7, 128, 1]]
      out_shape - [[7, 7, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[80], VSI_NN_OP_CONCAT, 4, 1, 138);
    node[80]->nn_param.concat.axis = 2;

    /*-----------------------------------------
      lid       - pool5/7x7_s1_139
      var       - node[81]
      name      - pool5/7x7_s1
      operation - pooling
      in_shape  - [[7, 7, 1024, 1]]
      out_shape - [[1, 1, 1024, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[81], VSI_NN_OP_POOL, 1, 1, 139);
    node[81]->nn_param.pool.ksize[0] = 7;
    node[81]->nn_param.pool.ksize[1] = 7;
    node[81]->nn_param.pool.stride[0] = 1;
    node[81]->nn_param.pool.stride[1] = 1;
    node[81]->nn_param.pool.pad[0] = 0;
    node[81]->nn_param.pool.pad[1] = 0;
    node[81]->nn_param.pool.pad[2] = 0;
    node[81]->nn_param.pool.pad[3] = 0;
    node[81]->nn_param.pool.type = VX_CONVOLUTIONAL_NETWORK_POOLING_AVG;
    node[81]->nn_param.pool.round_type = VSI_NN_ROUND_CEIL;
    node[81]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING;

    /*-----------------------------------------
      lid       - trans_loss3/classifier_141
      var       - node[82]
      name      - fullconnectrelu
      operation - fullconnectrelu
      in_shape  - [[1, 1, 1024, 1]]
      out_shape - [[1000, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[82], VSI_NN_OP_FCL_RELU, 3, 1, 141);
    node[82]->nn_param.fcl.weights = 1000;
    node[82]->vx_param.has_relu = FALSE;
    node[82]->vx_param.overflow_policy = VX_CONVERT_POLICY_WRAP;
    node[82]->vx_param.rounding_policy = VX_ROUND_POLICY_TO_ZERO;
    node[82]->vx_param.down_scale_size_rounding = VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR;

    /*-----------------------------------------
      lid       - prob_142
      var       - node[83]
      name      - prob
      operation - softmax
      in_shape  - [[1000, 1]]
      out_shape - [[1000, 1]]
    -----------------------------------------*/
    NEW_VXNODE(node[83], VSI_NN_OP_SOFTMAX, 1, 1, 142);


/*-----------------------------------------
  Tensor initialize
 -----------------------------------------*/
    attr.dtype.fmt = VSI_NN_DIM_FMT_NCHW;
    /* @data_0:out0 */
    attr.size[0] = 224;
    attr.size[1] = 224;
    attr.size[2] = 3;
    attr.size[3] = 1;
    attr.dim_num = 4;
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_NORM_TENSOR(norm_tensor[0], attr, VSI_NN_TYPE_INT8);

    /* @output_143:out0 */
    attr.size[0] = 1000;
    attr.size[1] = 1;
    attr.dim_num = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_NONE;
    NEW_NORM_TENSOR(norm_tensor[1], attr, VSI_NN_TYPE_FLOAT16);



    /* @conv1/7x7_s2_1_conv1/relu_7x7_2:weight */
    attr.size[0] = 7;
    attr.size[1] = 7;
    attr.size[2] = 3;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[0], attr, VSI_NN_TYPE_INT8, 256, 9408);

    /* @conv1/7x7_s2_1_conv1/relu_7x7_2:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[1], attr, VSI_NN_TYPE_INT32, 0, 256);

    /* @conv2/3x3_reduce_5_conv2/relu_3x3_reduce_6:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 64;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[2], attr, VSI_NN_TYPE_INT8, 121280, 4096);

    /* @conv2/3x3_reduce_5_conv2/relu_3x3_reduce_6:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[3], attr, VSI_NN_TYPE_INT32, 121024, 256);

    /* @conv2/3x3_7_conv2/relu_3x3_8:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 64;
    attr.size[3] = 192;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[4], attr, VSI_NN_TYPE_INT8, 10432, 110592);

    /* @conv2/3x3_7_conv2/relu_3x3_8:bias */
    attr.size[0] = 192;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[5], attr, VSI_NN_TYPE_INT32, 9664, 768);

    /* @inception_3a/1x1_11_inception_3a/relu_1x1_12:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 192;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[6], attr, VSI_NN_TYPE_INT8, 125632, 12288);

    /* @inception_3a/1x1_11_inception_3a/relu_1x1_12:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[7], attr, VSI_NN_TYPE_INT32, 125376, 256);

    /* @inception_3a/3x3_reduce_13_inception_3a/relu_3x3_reduce_14:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 192;
    attr.size[3] = 96;
    attr.dim_num = 4;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[8], attr, VSI_NN_TYPE_INT8, 249408, 18432);

    /* @inception_3a/3x3_reduce_13_inception_3a/relu_3x3_reduce_14:bias */
    attr.size[0] = 96;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[9], attr, VSI_NN_TYPE_INT32, 249024, 384);

    /* @inception_3a/5x5_reduce_17_inception_3a/relu_5x5_reduce_18:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 192;
    attr.size[3] = 16;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[10], attr, VSI_NN_TYPE_INT8, 280832, 3072);

    /* @inception_3a/5x5_reduce_17_inception_3a/relu_5x5_reduce_18:bias */
    attr.size[0] = 16;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[11], attr, VSI_NN_TYPE_INT32, 280768, 64);

    /* @inception_3a/pool_proj_22_inception_3a/relu_pool_proj_23:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 192;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[12], attr, VSI_NN_TYPE_INT8, 284032, 6144);

    /* @inception_3a/pool_proj_22_inception_3a/relu_pool_proj_23:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[13], attr, VSI_NN_TYPE_INT32, 283904, 128);

    /* @inception_3a/3x3_15_inception_3a/relu_3x3_16:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 96;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[14], attr, VSI_NN_TYPE_INT8, 138432, 110592);

    /* @inception_3a/3x3_15_inception_3a/relu_3x3_16:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[15], attr, VSI_NN_TYPE_INT32, 137920, 512);

    /* @inception_3a/5x5_19_inception_3a/relu_5x5_20:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 16;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[16], attr, VSI_NN_TYPE_INT8, 267968, 12800);

    /* @inception_3a/5x5_19_inception_3a/relu_5x5_20:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[17], attr, VSI_NN_TYPE_INT32, 267840, 128);

    /* @inception_3b/1x1_25_inception_3b/relu_1x1_26:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[18], attr, VSI_NN_TYPE_INT8, 290688, 32768);

    /* @inception_3b/1x1_25_inception_3b/relu_1x1_26:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[19], attr, VSI_NN_TYPE_INT32, 290176, 512);

    /* @inception_3b/3x3_reduce_27_inception_3b/relu_3x3_reduce_28:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[20], attr, VSI_NN_TYPE_INT8, 545920, 32768);

    /* @inception_3b/3x3_reduce_27_inception_3b/relu_3x3_reduce_28:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[21], attr, VSI_NN_TYPE_INT32, 545408, 512);

    /* @inception_3b/5x5_reduce_31_inception_3b/relu_5x5_reduce_32:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[22], attr, VSI_NN_TYPE_INT8, 656000, 8192);

    /* @inception_3b/5x5_reduce_31_inception_3b/relu_5x5_reduce_32:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[23], attr, VSI_NN_TYPE_INT32, 655872, 128);

    /* @inception_3b/pool_proj_36_inception_3b/relu_pool_proj_37:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 256;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[24], attr, VSI_NN_TYPE_INT8, 664448, 16384);

    /* @inception_3b/pool_proj_36_inception_3b/relu_pool_proj_37:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[25], attr, VSI_NN_TYPE_INT32, 664192, 256);

    /* @inception_3b/3x3_29_inception_3b/relu_3x3_30:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 128;
    attr.size[3] = 192;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[26], attr, VSI_NN_TYPE_INT8, 324224, 221184);

    /* @inception_3b/3x3_29_inception_3b/relu_3x3_30:bias */
    attr.size[0] = 192;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[27], attr, VSI_NN_TYPE_INT32, 323456, 768);

    /* @inception_3b/5x5_33_inception_3b/relu_5x5_34:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 32;
    attr.size[3] = 96;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[28], attr, VSI_NN_TYPE_INT8, 579072, 76800);

    /* @inception_3b/5x5_33_inception_3b/relu_5x5_34:bias */
    attr.size[0] = 96;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[29], attr, VSI_NN_TYPE_INT32, 578688, 384);

    /* @inception_4a/1x1_40_inception_4a/relu_1x1_41:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 480;
    attr.size[3] = 192;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[30], attr, VSI_NN_TYPE_INT8, 681600, 92160);

    /* @inception_4a/1x1_40_inception_4a/relu_1x1_41:bias */
    attr.size[0] = 192;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[31], attr, VSI_NN_TYPE_INT32, 680832, 768);

    /* @inception_4a/3x3_reduce_42_inception_4a/relu_3x3_reduce_43:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 480;
    attr.size[3] = 96;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[32], attr, VSI_NN_TYPE_INT8, 954688, 46080);

    /* @inception_4a/3x3_reduce_42_inception_4a/relu_3x3_reduce_43:bias */
    attr.size[0] = 96;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[33], attr, VSI_NN_TYPE_INT32, 954304, 384);

    /* @inception_4a/5x5_reduce_46_inception_4a/relu_5x5_reduce_47:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 480;
    attr.size[3] = 16;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[34], attr, VSI_NN_TYPE_INT8, 1020224, 7680);

    /* @inception_4a/5x5_reduce_46_inception_4a/relu_5x5_reduce_47:bias */
    attr.size[0] = 16;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[35], attr, VSI_NN_TYPE_INT32, 1020160, 64);

    /* @inception_4a/pool_proj_51_inception_4a/relu_pool_proj_52:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 480;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[36], attr, VSI_NN_TYPE_INT8, 1028160, 30720);

    /* @inception_4a/pool_proj_51_inception_4a/relu_pool_proj_52:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[37], attr, VSI_NN_TYPE_INT32, 1027904, 256);

    /* @inception_4a/3x3_44_inception_4a/relu_3x3_45:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 96;
    attr.size[3] = 208;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[38], attr, VSI_NN_TYPE_INT8, 774592, 179712);

    /* @inception_4a/3x3_44_inception_4a/relu_3x3_45:bias */
    attr.size[0] = 208;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[39], attr, VSI_NN_TYPE_INT32, 773760, 832);

    /* @inception_4a/5x5_48_inception_4a/relu_5x5_49:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 16;
    attr.size[3] = 48;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[40], attr, VSI_NN_TYPE_INT8, 1000960, 19200);

    /* @inception_4a/5x5_48_inception_4a/relu_5x5_49:bias */
    attr.size[0] = 48;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[41], attr, VSI_NN_TYPE_INT32, 1000768, 192);

    /* @inception_4b/1x1_54_inception_4b/relu_1x1_55:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 160;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[42], attr, VSI_NN_TYPE_INT8, 1059520, 81920);

    /* @inception_4b/1x1_54_inception_4b/relu_1x1_55:bias */
    attr.size[0] = 160;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[43], attr, VSI_NN_TYPE_INT32, 1058880, 640);

    /* @inception_4b/3x3_reduce_56_inception_4b/relu_3x3_reduce_57:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 112;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[44], attr, VSI_NN_TYPE_INT8, 1368576, 57344);

    /* @inception_4b/3x3_reduce_56_inception_4b/relu_3x3_reduce_57:bias */
    attr.size[0] = 112;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[45], attr, VSI_NN_TYPE_INT32, 1368128, 448);

    /* @inception_4b/5x5_reduce_60_inception_4b/relu_5x5_reduce_61:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 24;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[46], attr, VSI_NN_TYPE_INT8, 1464672, 12288);

    /* @inception_4b/5x5_reduce_60_inception_4b/relu_5x5_reduce_61:bias */
    attr.size[0] = 24;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[47], attr, VSI_NN_TYPE_INT32, 1464576, 96);

    /* @inception_4b/pool_proj_65_inception_4b/relu_pool_proj_66:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[48], attr, VSI_NN_TYPE_INT8, 1477216, 32768);

    /* @inception_4b/pool_proj_65_inception_4b/relu_pool_proj_66:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[49], attr, VSI_NN_TYPE_INT32, 1476960, 256);

    /* @inception_4b/3x3_58_inception_4b/relu_3x3_59:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 112;
    attr.size[3] = 224;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[50], attr, VSI_NN_TYPE_INT8, 1142336, 225792);

    /* @inception_4b/3x3_58_inception_4b/relu_3x3_59:bias */
    attr.size[0] = 224;
    attr.dim_num = 1;
    attr.dtype.fl = 4;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[51], attr, VSI_NN_TYPE_INT32, 1141440, 896);

    /* @inception_4b/5x5_62_inception_4b/relu_5x5_63:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 24;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[52], attr, VSI_NN_TYPE_INT8, 1426176, 38400);

    /* @inception_4b/5x5_62_inception_4b/relu_5x5_63:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[53], attr, VSI_NN_TYPE_INT32, 1425920, 256);

    /* @inception_4c/1x1_68_inception_4c/relu_1x1_69:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[54], attr, VSI_NN_TYPE_INT8, 1510496, 65536);

    /* @inception_4c/1x1_68_inception_4c/relu_1x1_69:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[55], attr, VSI_NN_TYPE_INT32, 1509984, 512);

    /* @inception_4c/3x3_reduce_70_inception_4c/relu_3x3_reduce_71:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[56], attr, VSI_NN_TYPE_INT8, 1872480, 65536);

    /* @inception_4c/3x3_reduce_70_inception_4c/relu_3x3_reduce_71:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[57], attr, VSI_NN_TYPE_INT32, 1871968, 512);

    /* @inception_4c/5x5_reduce_74_inception_4c/relu_5x5_reduce_75:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 24;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[58], attr, VSI_NN_TYPE_INT8, 1976768, 12288);

    /* @inception_4c/5x5_reduce_74_inception_4c/relu_5x5_reduce_75:bias */
    attr.size[0] = 24;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[59], attr, VSI_NN_TYPE_INT32, 1976672, 96);

    /* @inception_4c/pool_proj_79_inception_4c/relu_pool_proj_80:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[60], attr, VSI_NN_TYPE_INT8, 1989312, 32768);

    /* @inception_4c/pool_proj_79_inception_4c/relu_pool_proj_80:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[61], attr, VSI_NN_TYPE_INT32, 1989056, 256);

    /* @inception_4c/3x3_72_inception_4c/relu_3x3_73:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 128;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[62], attr, VSI_NN_TYPE_INT8, 1577056, 294912);

    /* @inception_4c/3x3_72_inception_4c/relu_3x3_73:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[63], attr, VSI_NN_TYPE_INT32, 1576032, 1024);

    /* @inception_4c/5x5_76_inception_4c/relu_5x5_77:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 24;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[64], attr, VSI_NN_TYPE_INT8, 1938272, 38400);

    /* @inception_4c/5x5_76_inception_4c/relu_5x5_77:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[65], attr, VSI_NN_TYPE_INT32, 1938016, 256);

    /* @inception_4d/1x1_82_inception_4d/relu_1x1_83:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 112;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[66], attr, VSI_NN_TYPE_INT8, 2022528, 57344);

    /* @inception_4d/1x1_82_inception_4d/relu_1x1_83:bias */
    attr.size[0] = 112;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[67], attr, VSI_NN_TYPE_INT32, 2022080, 448);

    /* @inception_4d/3x3_reduce_84_inception_4d/relu_3x3_reduce_85:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 144;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[68], attr, VSI_NN_TYPE_INT8, 2454848, 73728);

    /* @inception_4d/3x3_reduce_84_inception_4d/relu_3x3_reduce_85:bias */
    attr.size[0] = 144;
    attr.dim_num = 1;
    attr.dtype.fl = 5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[69], attr, VSI_NN_TYPE_INT32, 2454272, 576);

    /* @inception_4d/5x5_reduce_88_inception_4d/relu_5x5_reduce_89:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[70], attr, VSI_NN_TYPE_INT8, 2580160, 16384);

    /* @inception_4d/5x5_reduce_88_inception_4d/relu_5x5_reduce_89:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[71], attr, VSI_NN_TYPE_INT32, 2580032, 128);

    /* @inception_4d/pool_proj_93_inception_4d/relu_pool_proj_94:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 512;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[72], attr, VSI_NN_TYPE_INT8, 2596800, 32768);

    /* @inception_4d/pool_proj_93_inception_4d/relu_pool_proj_94:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[73], attr, VSI_NN_TYPE_INT32, 2596544, 256);

    /* @inception_4d/3x3_86_inception_4d/relu_3x3_87:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 144;
    attr.size[3] = 288;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[74], attr, VSI_NN_TYPE_INT8, 2081024, 373248);

    /* @inception_4d/3x3_86_inception_4d/relu_3x3_87:bias */
    attr.size[0] = 288;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[75], attr, VSI_NN_TYPE_INT32, 2079872, 1152);

    /* @inception_4d/5x5_90_inception_4d/relu_5x5_91:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 32;
    attr.size[3] = 64;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[76], attr, VSI_NN_TYPE_INT8, 2528832, 51200);

    /* @inception_4d/5x5_90_inception_4d/relu_5x5_91:bias */
    attr.size[0] = 64;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[77], attr, VSI_NN_TYPE_INT32, 2528576, 256);

    /* @inception_4e/1x1_96_inception_4e/relu_1x1_97:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 528;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[78], attr, VSI_NN_TYPE_INT8, 2630592, 135168);

    /* @inception_4e/1x1_96_inception_4e/relu_1x1_97:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[79], attr, VSI_NN_TYPE_INT32, 2629568, 1024);

    /* @inception_4e/3x3_reduce_98_inception_4e/relu_3x3_reduce_99:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 528;
    attr.size[3] = 160;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[80], attr, VSI_NN_TYPE_INT8, 3228480, 84480);

    /* @inception_4e/3x3_reduce_98_inception_4e/relu_3x3_reduce_99:bias */
    attr.size[0] = 160;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[81], attr, VSI_NN_TYPE_INT32, 3227840, 640);

    /* @inception_4e/5x5_reduce_102_inception_4e/relu_5x5_reduce_103:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 528;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[82], attr, VSI_NN_TYPE_INT8, 3416000, 16896);

    /* @inception_4e/5x5_reduce_102_inception_4e/relu_5x5_reduce_103:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[83], attr, VSI_NN_TYPE_INT32, 3415872, 128);

    /* @inception_4e/pool_proj_107_inception_4e/relu_pool_proj_108:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 528;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[84], attr, VSI_NN_TYPE_INT8, 3433408, 67584);

    /* @inception_4e/pool_proj_107_inception_4e/relu_pool_proj_108:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[85], attr, VSI_NN_TYPE_INT32, 3432896, 512);

    /* @inception_4e/3x3_100_inception_4e/relu_3x3_101:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 160;
    attr.size[3] = 320;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[86], attr, VSI_NN_TYPE_INT8, 2767040, 460800);

    /* @inception_4e/3x3_100_inception_4e/relu_3x3_101:bias */
    attr.size[0] = 320;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[87], attr, VSI_NN_TYPE_INT32, 2765760, 1280);

    /* @inception_4e/5x5_104_inception_4e/relu_5x5_105:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 32;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[88], attr, VSI_NN_TYPE_INT8, 3313472, 102400);

    /* @inception_4e/5x5_104_inception_4e/relu_5x5_105:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[89], attr, VSI_NN_TYPE_INT32, 3312960, 512);

    /* @inception_5a/1x1_111_inception_5a/relu_1x1_112:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 256;
    attr.dim_num = 4;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[90], attr, VSI_NN_TYPE_INT8, 3502016, 212992);

    /* @inception_5a/1x1_111_inception_5a/relu_1x1_112:bias */
    attr.size[0] = 256;
    attr.dim_num = 1;
    attr.dtype.fl = 6;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[91], attr, VSI_NN_TYPE_INT32, 3500992, 1024);

    /* @inception_5a/3x3_reduce_113_inception_5a/relu_3x3_reduce_114:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 160;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[92], attr, VSI_NN_TYPE_INT8, 4177728, 133120);

    /* @inception_5a/3x3_reduce_113_inception_5a/relu_3x3_reduce_114:bias */
    attr.size[0] = 160;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[93], attr, VSI_NN_TYPE_INT32, 4177088, 640);

    /* @inception_5a/5x5_reduce_117_inception_5a/relu_5x5_reduce_118:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 32;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[94], attr, VSI_NN_TYPE_INT8, 4413888, 26624);

    /* @inception_5a/5x5_reduce_117_inception_5a/relu_5x5_reduce_118:bias */
    attr.size[0] = 32;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[95], attr, VSI_NN_TYPE_INT32, 4413760, 128);

    /* @inception_5a/pool_proj_122_inception_5a/relu_pool_proj_123:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[96], attr, VSI_NN_TYPE_INT8, 4441024, 106496);

    /* @inception_5a/pool_proj_122_inception_5a/relu_pool_proj_123:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 7;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[97], attr, VSI_NN_TYPE_INT32, 4440512, 512);

    /* @inception_5a/3x3_115_inception_5a/relu_3x3_116:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 160;
    attr.size[3] = 320;
    attr.dim_num = 4;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[98], attr, VSI_NN_TYPE_INT8, 3716288, 460800);

    /* @inception_5a/3x3_115_inception_5a/relu_3x3_116:bias */
    attr.size[0] = 320;
    attr.dim_num = 1;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[99], attr, VSI_NN_TYPE_INT32, 3715008, 1280);

    /* @inception_5a/5x5_119_inception_5a/relu_5x5_120:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 32;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[100], attr, VSI_NN_TYPE_INT8, 4311360, 102400);

    /* @inception_5a/5x5_119_inception_5a/relu_5x5_120:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[101], attr, VSI_NN_TYPE_INT32, 4310848, 512);

    /* @inception_5b/1x1_125_inception_5b/relu_1x1_126:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 384;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[102], attr, VSI_NN_TYPE_INT8, 4549056, 319488);

    /* @inception_5b/1x1_125_inception_5b/relu_1x1_126:bias */
    attr.size[0] = 384;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[103], attr, VSI_NN_TYPE_INT32, 4547520, 1536);

    /* @inception_5b/3x3_reduce_127_inception_5b/relu_3x3_reduce_128:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 192;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[104], attr, VSI_NN_TYPE_INT8, 5534400, 159744);

    /* @inception_5b/3x3_reduce_127_inception_5b/relu_3x3_reduce_128:bias */
    attr.size[0] = 192;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[105], attr, VSI_NN_TYPE_INT32, 5533632, 768);

    /* @inception_5b/5x5_reduce_131_inception_5b/relu_5x5_reduce_132:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 48;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[106], attr, VSI_NN_TYPE_INT8, 5848448, 39936);

    /* @inception_5b/5x5_reduce_131_inception_5b/relu_5x5_reduce_132:bias */
    attr.size[0] = 48;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[107], attr, VSI_NN_TYPE_INT32, 5848256, 192);

    /* @inception_5b/pool_proj_136_inception_5b/relu_pool_proj_137:weight */
    attr.size[0] = 1;
    attr.size[1] = 1;
    attr.size[2] = 832;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[108], attr, VSI_NN_TYPE_INT8, 5888896, 106496);

    /* @inception_5b/pool_proj_136_inception_5b/relu_pool_proj_137:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 8;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[109], attr, VSI_NN_TYPE_INT32, 5888384, 512);

    /* @inception_5b/3x3_129_inception_5b/relu_3x3_130:weight */
    attr.size[0] = 3;
    attr.size[1] = 3;
    attr.size[2] = 192;
    attr.size[3] = 384;
    attr.dim_num = 4;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[110], attr, VSI_NN_TYPE_INT8, 4870080, 663552);

    /* @inception_5b/3x3_129_inception_5b/relu_3x3_130:bias */
    attr.size[0] = 384;
    attr.dim_num = 1;
    attr.dtype.fl = 11;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[111], attr, VSI_NN_TYPE_INT32, 4868544, 1536);

    /* @inception_5b/5x5_133_inception_5b/relu_5x5_134:weight */
    attr.size[0] = 5;
    attr.size[1] = 5;
    attr.size[2] = 48;
    attr.size[3] = 128;
    attr.dim_num = 4;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[112], attr, VSI_NN_TYPE_INT8, 5694656, 153600);

    /* @inception_5b/5x5_133_inception_5b/relu_5x5_134:bias */
    attr.size[0] = 128;
    attr.dim_num = 1;
    attr.dtype.fl = 10;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[113], attr, VSI_NN_TYPE_INT32, 5694144, 512);

    /* @trans_loss3/classifier_141:weight */
    attr.size[0] = 1024;
    attr.size[1] = 1000;
    attr.dim_num = 2;
    attr.dtype.fl = 9;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[114], attr, VSI_NN_TYPE_INT8, 5999392, 1024000);

    /* @trans_loss3/classifier_141:bias */
    attr.size[0] = 1000;
    attr.dim_num = 1;
    attr.dtype.fl = 12;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_CONST_TENSOR(const_tensor[115], attr, VSI_NN_TYPE_INT32, 5995392, 4000);



    /* @conv1/7x7_s2_1_conv1/relu_7x7_2:out0 */
    attr.dtype.fl = -5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[0]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool1/3x3_s2_3:out0 */
    attr.dtype.fl = -5;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[1]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool1/norm1_4:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[2]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2/3x3_reduce_5_conv2/relu_3x3_reduce_6:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[3]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2/3x3_7_conv2/relu_3x3_8:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[4]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @conv2/norm2_9:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[5]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool2/3x3_s2_10:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[6]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/pool_21:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[7]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/1x1_11_inception_3a/relu_1x1_12:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[8]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/3x3_reduce_13_inception_3a/relu_3x3_reduce_14:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[9]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/5x5_reduce_17_inception_3a/relu_5x5_reduce_18:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[10]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/pool_proj_22_inception_3a/relu_pool_proj_23:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[11]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/3x3_15_inception_3a/relu_3x3_16:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[12]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/5x5_19_inception_3a/relu_5x5_20:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[13]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3a/output_24:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[14]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/pool_35:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[15]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/1x1_25_inception_3b/relu_1x1_26:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[16]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/3x3_reduce_27_inception_3b/relu_3x3_reduce_28:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[17]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/5x5_reduce_31_inception_3b/relu_5x5_reduce_32:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[18]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/pool_proj_36_inception_3b/relu_pool_proj_37:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[19]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/3x3_29_inception_3b/relu_3x3_30:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[20]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/5x5_33_inception_3b/relu_5x5_34:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[21]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_3b/output_38:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[22]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool3/3x3_s2_39:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[23]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/pool_50:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[24]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/1x1_40_inception_4a/relu_1x1_41:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[25]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/3x3_reduce_42_inception_4a/relu_3x3_reduce_43:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[26]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/5x5_reduce_46_inception_4a/relu_5x5_reduce_47:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[27]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/pool_proj_51_inception_4a/relu_pool_proj_52:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[28]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/3x3_44_inception_4a/relu_3x3_45:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[29]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/5x5_48_inception_4a/relu_5x5_49:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[30]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4a/output_53:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[31]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/pool_64:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[32]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/1x1_54_inception_4b/relu_1x1_55:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[33]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/3x3_reduce_56_inception_4b/relu_3x3_reduce_57:out0 */
    attr.dtype.fl = -3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[34]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/5x5_reduce_60_inception_4b/relu_5x5_reduce_61:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[35]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/pool_proj_65_inception_4b/relu_pool_proj_66:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[36]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/3x3_58_inception_4b/relu_3x3_59:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[37]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/5x5_62_inception_4b/relu_5x5_63:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[38]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4b/output_67:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[39]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/pool_78:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[40]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/1x1_68_inception_4c/relu_1x1_69:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[41]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/3x3_reduce_70_inception_4c/relu_3x3_reduce_71:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[42]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/5x5_reduce_74_inception_4c/relu_5x5_reduce_75:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[43]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/pool_proj_79_inception_4c/relu_pool_proj_80:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[44]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/3x3_72_inception_4c/relu_3x3_73:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[45]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/5x5_76_inception_4c/relu_5x5_77:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[46]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4c/output_81:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[47]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/pool_92:out0 */
    attr.dtype.fl = -2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[48]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/1x1_82_inception_4d/relu_1x1_83:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[49]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/3x3_reduce_84_inception_4d/relu_3x3_reduce_85:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[50]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/5x5_reduce_88_inception_4d/relu_5x5_reduce_89:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[51]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/pool_proj_93_inception_4d/relu_pool_proj_94:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[52]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/3x3_86_inception_4d/relu_3x3_87:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[53]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/5x5_90_inception_4d/relu_5x5_91:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[54]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4d/output_95:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[55]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/pool_106:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[56]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/1x1_96_inception_4e/relu_1x1_97:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[57]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/3x3_reduce_98_inception_4e/relu_3x3_reduce_99:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[58]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/5x5_reduce_102_inception_4e/relu_5x5_reduce_103:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[59]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/pool_proj_107_inception_4e/relu_pool_proj_108:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[60]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/3x3_100_inception_4e/relu_3x3_101:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[61]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/5x5_104_inception_4e/relu_5x5_105:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[62]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_4e/output_109:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[63]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool4/3x3_s2_110:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[64]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/pool_121:out0 */
    attr.dtype.fl = -1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[65]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/1x1_111_inception_5a/relu_1x1_112:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[66]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/3x3_reduce_113_inception_5a/relu_3x3_reduce_114:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[67]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/5x5_reduce_117_inception_5a/relu_5x5_reduce_118:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[68]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/pool_proj_122_inception_5a/relu_pool_proj_123:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[69]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/3x3_115_inception_5a/relu_3x3_116:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[70]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/5x5_119_inception_5a/relu_5x5_120:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[71]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5a/output_124:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[72]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/pool_135:out0 */
    attr.dtype.fl = 0;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[73]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/1x1_125_inception_5b/relu_1x1_126:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[74]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/3x3_reduce_127_inception_5b/relu_3x3_reduce_128:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[75]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/5x5_reduce_131_inception_5b/relu_5x5_reduce_132:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[76]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/pool_proj_136_inception_5b/relu_pool_proj_137:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[77]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/3x3_129_inception_5b/relu_3x3_130:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[78]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/5x5_133_inception_5b/relu_5x5_134:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[79]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @inception_5b/output_138:out0 */
    attr.dtype.fl = 1;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[80]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @pool5/7x7_s1_139:out0 */
    attr.dtype.fl = 3;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[81]->output.tensors[0], attr, VSI_NN_TYPE_INT8);

    /* @trans_loss3/classifier_141:out0 */
    attr.dtype.fl = 2;
    attr.dtype.qnt_type = VSI_NN_QNT_TYPE_DFP;
    NEW_VIRTUAL_TENSOR(node[82]->output.tensors[0], attr, VSI_NN_TYPE_INT8);



/*-----------------------------------------
  Connection initialize
 -----------------------------------------*/
    node[0]->input.tensors[0] = norm_tensor[0];
    node[83]->output.tensors[0] = norm_tensor[1];

    /* conv1/7x7_s2_1_conv1/relu_7x7_2 */
    node[0]->input.tensors[1] = const_tensor[0]; /* data_weight */
    node[0]->input.tensors[2] = const_tensor[1]; /* data_bias */

    /* pool1/3x3_s2_3 */
    node[1]->input.tensors[0] = node[0]->output.tensors[0];

    /* pool1/norm1_4 */
    node[2]->input.tensors[0] = node[1]->output.tensors[0];

    /* conv2/3x3_reduce_5_conv2/relu_3x3_reduce_6 */
    node[3]->input.tensors[0] = node[2]->output.tensors[0];
    node[3]->input.tensors[1] = const_tensor[2]; /* data_weight */
    node[3]->input.tensors[2] = const_tensor[3]; /* data_bias */

    /* conv2/3x3_7_conv2/relu_3x3_8 */
    node[4]->input.tensors[0] = node[3]->output.tensors[0];
    node[4]->input.tensors[1] = const_tensor[4]; /* data_weight */
    node[4]->input.tensors[2] = const_tensor[5]; /* data_bias */

    /* conv2/norm2_9 */
    node[5]->input.tensors[0] = node[4]->output.tensors[0];

    /* pool2/3x3_s2_10 */
    node[6]->input.tensors[0] = node[5]->output.tensors[0];

    /* inception_3a/pool_21 */
    node[7]->input.tensors[0] = node[6]->output.tensors[0];

    /* inception_3a/1x1_11_inception_3a/relu_1x1_12 */
    node[8]->input.tensors[0] = node[6]->output.tensors[0];
    node[8]->input.tensors[1] = const_tensor[6]; /* data_weight */
    node[8]->input.tensors[2] = const_tensor[7]; /* data_bias */

    /* inception_3a/3x3_reduce_13_inception_3a/relu_3x3_reduce_14 */
    node[9]->input.tensors[0] = node[6]->output.tensors[0];
    node[9]->input.tensors[1] = const_tensor[8]; /* data_weight */
    node[9]->input.tensors[2] = const_tensor[9]; /* data_bias */

    /* inception_3a/5x5_reduce_17_inception_3a/relu_5x5_reduce_18 */
    node[10]->input.tensors[0] = node[6]->output.tensors[0];
    node[10]->input.tensors[1] = const_tensor[10]; /* data_weight */
    node[10]->input.tensors[2] = const_tensor[11]; /* data_bias */

    /* inception_3a/pool_proj_22_inception_3a/relu_pool_proj_23 */
    node[11]->input.tensors[0] = node[7]->output.tensors[0];
    node[11]->input.tensors[1] = const_tensor[12]; /* data_weight */
    node[11]->input.tensors[2] = const_tensor[13]; /* data_bias */

    /* inception_3a/3x3_15_inception_3a/relu_3x3_16 */
    node[12]->input.tensors[0] = node[9]->output.tensors[0];
    node[12]->input.tensors[1] = const_tensor[14]; /* data_weight */
    node[12]->input.tensors[2] = const_tensor[15]; /* data_bias */

    /* inception_3a/5x5_19_inception_3a/relu_5x5_20 */
    node[13]->input.tensors[0] = node[10]->output.tensors[0];
    node[13]->input.tensors[1] = const_tensor[16]; /* data_weight */
    node[13]->input.tensors[2] = const_tensor[17]; /* data_bias */

    /* inception_3a/output_24 */
    node[14]->input.tensors[0] = node[8]->output.tensors[0];
    node[14]->input.tensors[1] = node[12]->output.tensors[0];
    node[14]->input.tensors[2] = node[13]->output.tensors[0];
    node[14]->input.tensors[3] = node[11]->output.tensors[0];

    /* inception_3b/pool_35 */
    node[15]->input.tensors[0] = node[14]->output.tensors[0];

    /* inception_3b/1x1_25_inception_3b/relu_1x1_26 */
    node[16]->input.tensors[0] = node[14]->output.tensors[0];
    node[16]->input.tensors[1] = const_tensor[18]; /* data_weight */
    node[16]->input.tensors[2] = const_tensor[19]; /* data_bias */

    /* inception_3b/3x3_reduce_27_inception_3b/relu_3x3_reduce_28 */
    node[17]->input.tensors[0] = node[14]->output.tensors[0];
    node[17]->input.tensors[1] = const_tensor[20]; /* data_weight */
    node[17]->input.tensors[2] = const_tensor[21]; /* data_bias */

    /* inception_3b/5x5_reduce_31_inception_3b/relu_5x5_reduce_32 */
    node[18]->input.tensors[0] = node[14]->output.tensors[0];
    node[18]->input.tensors[1] = const_tensor[22]; /* data_weight */
    node[18]->input.tensors[2] = const_tensor[23]; /* data_bias */

    /* inception_3b/pool_proj_36_inception_3b/relu_pool_proj_37 */
    node[19]->input.tensors[0] = node[15]->output.tensors[0];
    node[19]->input.tensors[1] = const_tensor[24]; /* data_weight */
    node[19]->input.tensors[2] = const_tensor[25]; /* data_bias */

    /* inception_3b/3x3_29_inception_3b/relu_3x3_30 */
    node[20]->input.tensors[0] = node[17]->output.tensors[0];
    node[20]->input.tensors[1] = const_tensor[26]; /* data_weight */
    node[20]->input.tensors[2] = const_tensor[27]; /* data_bias */

    /* inception_3b/5x5_33_inception_3b/relu_5x5_34 */
    node[21]->input.tensors[0] = node[18]->output.tensors[0];
    node[21]->input.tensors[1] = const_tensor[28]; /* data_weight */
    node[21]->input.tensors[2] = const_tensor[29]; /* data_bias */

    /* inception_3b/output_38 */
    node[22]->input.tensors[0] = node[16]->output.tensors[0];
    node[22]->input.tensors[1] = node[20]->output.tensors[0];
    node[22]->input.tensors[2] = node[21]->output.tensors[0];
    node[22]->input.tensors[3] = node[19]->output.tensors[0];

    /* pool3/3x3_s2_39 */
    node[23]->input.tensors[0] = node[22]->output.tensors[0];

    /* inception_4a/pool_50 */
    node[24]->input.tensors[0] = node[23]->output.tensors[0];

    /* inception_4a/1x1_40_inception_4a/relu_1x1_41 */
    node[25]->input.tensors[0] = node[23]->output.tensors[0];
    node[25]->input.tensors[1] = const_tensor[30]; /* data_weight */
    node[25]->input.tensors[2] = const_tensor[31]; /* data_bias */

    /* inception_4a/3x3_reduce_42_inception_4a/relu_3x3_reduce_43 */
    node[26]->input.tensors[0] = node[23]->output.tensors[0];
    node[26]->input.tensors[1] = const_tensor[32]; /* data_weight */
    node[26]->input.tensors[2] = const_tensor[33]; /* data_bias */

    /* inception_4a/5x5_reduce_46_inception_4a/relu_5x5_reduce_47 */
    node[27]->input.tensors[0] = node[23]->output.tensors[0];
    node[27]->input.tensors[1] = const_tensor[34]; /* data_weight */
    node[27]->input.tensors[2] = const_tensor[35]; /* data_bias */

    /* inception_4a/pool_proj_51_inception_4a/relu_pool_proj_52 */
    node[28]->input.tensors[0] = node[24]->output.tensors[0];
    node[28]->input.tensors[1] = const_tensor[36]; /* data_weight */
    node[28]->input.tensors[2] = const_tensor[37]; /* data_bias */

    /* inception_4a/3x3_44_inception_4a/relu_3x3_45 */
    node[29]->input.tensors[0] = node[26]->output.tensors[0];
    node[29]->input.tensors[1] = const_tensor[38]; /* data_weight */
    node[29]->input.tensors[2] = const_tensor[39]; /* data_bias */

    /* inception_4a/5x5_48_inception_4a/relu_5x5_49 */
    node[30]->input.tensors[0] = node[27]->output.tensors[0];
    node[30]->input.tensors[1] = const_tensor[40]; /* data_weight */
    node[30]->input.tensors[2] = const_tensor[41]; /* data_bias */

    /* inception_4a/output_53 */
    node[31]->input.tensors[0] = node[25]->output.tensors[0];
    node[31]->input.tensors[1] = node[29]->output.tensors[0];
    node[31]->input.tensors[2] = node[30]->output.tensors[0];
    node[31]->input.tensors[3] = node[28]->output.tensors[0];

    /* inception_4b/pool_64 */
    node[32]->input.tensors[0] = node[31]->output.tensors[0];

    /* inception_4b/1x1_54_inception_4b/relu_1x1_55 */
    node[33]->input.tensors[0] = node[31]->output.tensors[0];
    node[33]->input.tensors[1] = const_tensor[42]; /* data_weight */
    node[33]->input.tensors[2] = const_tensor[43]; /* data_bias */

    /* inception_4b/3x3_reduce_56_inception_4b/relu_3x3_reduce_57 */
    node[34]->input.tensors[0] = node[31]->output.tensors[0];
    node[34]->input.tensors[1] = const_tensor[44]; /* data_weight */
    node[34]->input.tensors[2] = const_tensor[45]; /* data_bias */

    /* inception_4b/5x5_reduce_60_inception_4b/relu_5x5_reduce_61 */
    node[35]->input.tensors[0] = node[31]->output.tensors[0];
    node[35]->input.tensors[1] = const_tensor[46]; /* data_weight */
    node[35]->input.tensors[2] = const_tensor[47]; /* data_bias */

    /* inception_4b/pool_proj_65_inception_4b/relu_pool_proj_66 */
    node[36]->input.tensors[0] = node[32]->output.tensors[0];
    node[36]->input.tensors[1] = const_tensor[48]; /* data_weight */
    node[36]->input.tensors[2] = const_tensor[49]; /* data_bias */

    /* inception_4b/3x3_58_inception_4b/relu_3x3_59 */
    node[37]->input.tensors[0] = node[34]->output.tensors[0];
    node[37]->input.tensors[1] = const_tensor[50]; /* data_weight */
    node[37]->input.tensors[2] = const_tensor[51]; /* data_bias */

    /* inception_4b/5x5_62_inception_4b/relu_5x5_63 */
    node[38]->input.tensors[0] = node[35]->output.tensors[0];
    node[38]->input.tensors[1] = const_tensor[52]; /* data_weight */
    node[38]->input.tensors[2] = const_tensor[53]; /* data_bias */

    /* inception_4b/output_67 */
    node[39]->input.tensors[0] = node[33]->output.tensors[0];
    node[39]->input.tensors[1] = node[37]->output.tensors[0];
    node[39]->input.tensors[2] = node[38]->output.tensors[0];
    node[39]->input.tensors[3] = node[36]->output.tensors[0];

    /* inception_4c/pool_78 */
    node[40]->input.tensors[0] = node[39]->output.tensors[0];

    /* inception_4c/1x1_68_inception_4c/relu_1x1_69 */
    node[41]->input.tensors[0] = node[39]->output.tensors[0];
    node[41]->input.tensors[1] = const_tensor[54]; /* data_weight */
    node[41]->input.tensors[2] = const_tensor[55]; /* data_bias */

    /* inception_4c/3x3_reduce_70_inception_4c/relu_3x3_reduce_71 */
    node[42]->input.tensors[0] = node[39]->output.tensors[0];
    node[42]->input.tensors[1] = const_tensor[56]; /* data_weight */
    node[42]->input.tensors[2] = const_tensor[57]; /* data_bias */

    /* inception_4c/5x5_reduce_74_inception_4c/relu_5x5_reduce_75 */
    node[43]->input.tensors[0] = node[39]->output.tensors[0];
    node[43]->input.tensors[1] = const_tensor[58]; /* data_weight */
    node[43]->input.tensors[2] = const_tensor[59]; /* data_bias */

    /* inception_4c/pool_proj_79_inception_4c/relu_pool_proj_80 */
    node[44]->input.tensors[0] = node[40]->output.tensors[0];
    node[44]->input.tensors[1] = const_tensor[60]; /* data_weight */
    node[44]->input.tensors[2] = const_tensor[61]; /* data_bias */

    /* inception_4c/3x3_72_inception_4c/relu_3x3_73 */
    node[45]->input.tensors[0] = node[42]->output.tensors[0];
    node[45]->input.tensors[1] = const_tensor[62]; /* data_weight */
    node[45]->input.tensors[2] = const_tensor[63]; /* data_bias */

    /* inception_4c/5x5_76_inception_4c/relu_5x5_77 */
    node[46]->input.tensors[0] = node[43]->output.tensors[0];
    node[46]->input.tensors[1] = const_tensor[64]; /* data_weight */
    node[46]->input.tensors[2] = const_tensor[65]; /* data_bias */

    /* inception_4c/output_81 */
    node[47]->input.tensors[0] = node[41]->output.tensors[0];
    node[47]->input.tensors[1] = node[45]->output.tensors[0];
    node[47]->input.tensors[2] = node[46]->output.tensors[0];
    node[47]->input.tensors[3] = node[44]->output.tensors[0];

    /* inception_4d/pool_92 */
    node[48]->input.tensors[0] = node[47]->output.tensors[0];

    /* inception_4d/1x1_82_inception_4d/relu_1x1_83 */
    node[49]->input.tensors[0] = node[47]->output.tensors[0];
    node[49]->input.tensors[1] = const_tensor[66]; /* data_weight */
    node[49]->input.tensors[2] = const_tensor[67]; /* data_bias */

    /* inception_4d/3x3_reduce_84_inception_4d/relu_3x3_reduce_85 */
    node[50]->input.tensors[0] = node[47]->output.tensors[0];
    node[50]->input.tensors[1] = const_tensor[68]; /* data_weight */
    node[50]->input.tensors[2] = const_tensor[69]; /* data_bias */

    /* inception_4d/5x5_reduce_88_inception_4d/relu_5x5_reduce_89 */
    node[51]->input.tensors[0] = node[47]->output.tensors[0];
    node[51]->input.tensors[1] = const_tensor[70]; /* data_weight */
    node[51]->input.tensors[2] = const_tensor[71]; /* data_bias */

    /* inception_4d/pool_proj_93_inception_4d/relu_pool_proj_94 */
    node[52]->input.tensors[0] = node[48]->output.tensors[0];
    node[52]->input.tensors[1] = const_tensor[72]; /* data_weight */
    node[52]->input.tensors[2] = const_tensor[73]; /* data_bias */

    /* inception_4d/3x3_86_inception_4d/relu_3x3_87 */
    node[53]->input.tensors[0] = node[50]->output.tensors[0];
    node[53]->input.tensors[1] = const_tensor[74]; /* data_weight */
    node[53]->input.tensors[2] = const_tensor[75]; /* data_bias */

    /* inception_4d/5x5_90_inception_4d/relu_5x5_91 */
    node[54]->input.tensors[0] = node[51]->output.tensors[0];
    node[54]->input.tensors[1] = const_tensor[76]; /* data_weight */
    node[54]->input.tensors[2] = const_tensor[77]; /* data_bias */

    /* inception_4d/output_95 */
    node[55]->input.tensors[0] = node[49]->output.tensors[0];
    node[55]->input.tensors[1] = node[53]->output.tensors[0];
    node[55]->input.tensors[2] = node[54]->output.tensors[0];
    node[55]->input.tensors[3] = node[52]->output.tensors[0];

    /* inception_4e/pool_106 */
    node[56]->input.tensors[0] = node[55]->output.tensors[0];

    /* inception_4e/1x1_96_inception_4e/relu_1x1_97 */
    node[57]->input.tensors[0] = node[55]->output.tensors[0];
    node[57]->input.tensors[1] = const_tensor[78]; /* data_weight */
    node[57]->input.tensors[2] = const_tensor[79]; /* data_bias */

    /* inception_4e/3x3_reduce_98_inception_4e/relu_3x3_reduce_99 */
    node[58]->input.tensors[0] = node[55]->output.tensors[0];
    node[58]->input.tensors[1] = const_tensor[80]; /* data_weight */
    node[58]->input.tensors[2] = const_tensor[81]; /* data_bias */

    /* inception_4e/5x5_reduce_102_inception_4e/relu_5x5_reduce_103 */
    node[59]->input.tensors[0] = node[55]->output.tensors[0];
    node[59]->input.tensors[1] = const_tensor[82]; /* data_weight */
    node[59]->input.tensors[2] = const_tensor[83]; /* data_bias */

    /* inception_4e/pool_proj_107_inception_4e/relu_pool_proj_108 */
    node[60]->input.tensors[0] = node[56]->output.tensors[0];
    node[60]->input.tensors[1] = const_tensor[84]; /* data_weight */
    node[60]->input.tensors[2] = const_tensor[85]; /* data_bias */

    /* inception_4e/3x3_100_inception_4e/relu_3x3_101 */
    node[61]->input.tensors[0] = node[58]->output.tensors[0];
    node[61]->input.tensors[1] = const_tensor[86]; /* data_weight */
    node[61]->input.tensors[2] = const_tensor[87]; /* data_bias */

    /* inception_4e/5x5_104_inception_4e/relu_5x5_105 */
    node[62]->input.tensors[0] = node[59]->output.tensors[0];
    node[62]->input.tensors[1] = const_tensor[88]; /* data_weight */
    node[62]->input.tensors[2] = const_tensor[89]; /* data_bias */

    /* inception_4e/output_109 */
    node[63]->input.tensors[0] = node[57]->output.tensors[0];
    node[63]->input.tensors[1] = node[61]->output.tensors[0];
    node[63]->input.tensors[2] = node[62]->output.tensors[0];
    node[63]->input.tensors[3] = node[60]->output.tensors[0];

    /* pool4/3x3_s2_110 */
    node[64]->input.tensors[0] = node[63]->output.tensors[0];

    /* inception_5a/pool_121 */
    node[65]->input.tensors[0] = node[64]->output.tensors[0];

    /* inception_5a/1x1_111_inception_5a/relu_1x1_112 */
    node[66]->input.tensors[0] = node[64]->output.tensors[0];
    node[66]->input.tensors[1] = const_tensor[90]; /* data_weight */
    node[66]->input.tensors[2] = const_tensor[91]; /* data_bias */

    /* inception_5a/3x3_reduce_113_inception_5a/relu_3x3_reduce_114 */
    node[67]->input.tensors[0] = node[64]->output.tensors[0];
    node[67]->input.tensors[1] = const_tensor[92]; /* data_weight */
    node[67]->input.tensors[2] = const_tensor[93]; /* data_bias */

    /* inception_5a/5x5_reduce_117_inception_5a/relu_5x5_reduce_118 */
    node[68]->input.tensors[0] = node[64]->output.tensors[0];
    node[68]->input.tensors[1] = const_tensor[94]; /* data_weight */
    node[68]->input.tensors[2] = const_tensor[95]; /* data_bias */

    /* inception_5a/pool_proj_122_inception_5a/relu_pool_proj_123 */
    node[69]->input.tensors[0] = node[65]->output.tensors[0];
    node[69]->input.tensors[1] = const_tensor[96]; /* data_weight */
    node[69]->input.tensors[2] = const_tensor[97]; /* data_bias */

    /* inception_5a/3x3_115_inception_5a/relu_3x3_116 */
    node[70]->input.tensors[0] = node[67]->output.tensors[0];
    node[70]->input.tensors[1] = const_tensor[98]; /* data_weight */
    node[70]->input.tensors[2] = const_tensor[99]; /* data_bias */

    /* inception_5a/5x5_119_inception_5a/relu_5x5_120 */
    node[71]->input.tensors[0] = node[68]->output.tensors[0];
    node[71]->input.tensors[1] = const_tensor[100]; /* data_weight */
    node[71]->input.tensors[2] = const_tensor[101]; /* data_bias */

    /* inception_5a/output_124 */
    node[72]->input.tensors[0] = node[66]->output.tensors[0];
    node[72]->input.tensors[1] = node[70]->output.tensors[0];
    node[72]->input.tensors[2] = node[71]->output.tensors[0];
    node[72]->input.tensors[3] = node[69]->output.tensors[0];

    /* inception_5b/pool_135 */
    node[73]->input.tensors[0] = node[72]->output.tensors[0];

    /* inception_5b/1x1_125_inception_5b/relu_1x1_126 */
    node[74]->input.tensors[0] = node[72]->output.tensors[0];
    node[74]->input.tensors[1] = const_tensor[102]; /* data_weight */
    node[74]->input.tensors[2] = const_tensor[103]; /* data_bias */

    /* inception_5b/3x3_reduce_127_inception_5b/relu_3x3_reduce_128 */
    node[75]->input.tensors[0] = node[72]->output.tensors[0];
    node[75]->input.tensors[1] = const_tensor[104]; /* data_weight */
    node[75]->input.tensors[2] = const_tensor[105]; /* data_bias */

    /* inception_5b/5x5_reduce_131_inception_5b/relu_5x5_reduce_132 */
    node[76]->input.tensors[0] = node[72]->output.tensors[0];
    node[76]->input.tensors[1] = const_tensor[106]; /* data_weight */
    node[76]->input.tensors[2] = const_tensor[107]; /* data_bias */

    /* inception_5b/pool_proj_136_inception_5b/relu_pool_proj_137 */
    node[77]->input.tensors[0] = node[73]->output.tensors[0];
    node[77]->input.tensors[1] = const_tensor[108]; /* data_weight */
    node[77]->input.tensors[2] = const_tensor[109]; /* data_bias */

    /* inception_5b/3x3_129_inception_5b/relu_3x3_130 */
    node[78]->input.tensors[0] = node[75]->output.tensors[0];
    node[78]->input.tensors[1] = const_tensor[110]; /* data_weight */
    node[78]->input.tensors[2] = const_tensor[111]; /* data_bias */

    /* inception_5b/5x5_133_inception_5b/relu_5x5_134 */
    node[79]->input.tensors[0] = node[76]->output.tensors[0];
    node[79]->input.tensors[1] = const_tensor[112]; /* data_weight */
    node[79]->input.tensors[2] = const_tensor[113]; /* data_bias */

    /* inception_5b/output_138 */
    node[80]->input.tensors[0] = node[74]->output.tensors[0];
    node[80]->input.tensors[1] = node[78]->output.tensors[0];
    node[80]->input.tensors[2] = node[79]->output.tensors[0];
    node[80]->input.tensors[3] = node[77]->output.tensors[0];

    /* pool5/7x7_s1_139 */
    node[81]->input.tensors[0] = node[80]->output.tensors[0];

    /* trans_loss3/classifier_141 */
    node[82]->input.tensors[0] = node[81]->output.tensors[0];
    node[82]->input.tensors[1] = const_tensor[114]; /* data_weight */
    node[82]->input.tensors[2] = const_tensor[115]; /* data_bias */

    /* prob_142 */
    node[83]->input.tensors[0] = node[82]->output.tensors[0];



    graph->input.tensors[0] = norm_tensor[0];
    graph->output.tensors[0] = norm_tensor[1];


    status = vsi_nn_SetupGraph( graph, FALSE );
    if( VSI_FAILURE == status )
    {
        goto error;
    }

    fclose( fp );

    return graph;

error:
    if( NULL != fp )
    {
        fclose( fp );
    }

    release_ctx = ( NULL == in_ctx );
    vnn_ReleaseBvlcGooglenetCaffe( graph, release_ctx );

    return NULL;
} /* vsi_nn_CreateBvlcGooglenetCaffe() */

void vnn_ReleaseBvlcGooglenetCaffe
    (
    vsi_nn_graph_t * graph,
    vsi_bool release_ctx
    )
{
    vsi_nn_context_t ctx;
    if( NULL != graph )
    {
        ctx = graph->ctx;
        vsi_nn_ReleaseGraph( &graph );

        /*-----------------------------------------
        Unregister client ops
        -----------------------------------------*/
        

        if( release_ctx )
        {
            vsi_nn_ReleaseContext( &ctx );
        }
    }
} /* vsi_nn_ReleaseBvlcGooglenetCaffe() */

